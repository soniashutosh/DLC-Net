{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff008b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:25:01.356691: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:01.356725: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Citrus_data_report/import file')\n",
    "\n",
    "import model_import_file\n",
    "import evaluation_metrics_import_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87abc474",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"dataset\"\n",
    "path_to_save = \"Citrus_data_report/data_save_with_aug/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75aaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 23})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0569dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:25:05.284338: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:05.284460: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:05.284526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:05.284584: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:05.284641: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:05.284696: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:05.284751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:05.284808: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:25:05.284820: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-01 13:25:05.285295: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train : 2850\n",
      "Size of y_train : 2850\n",
      "Size of X_test : 121\n",
      "Size of y_test : 121\n",
      "Proposed Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 8s 792ms/step - loss: 0.3612 - accuracy: 0.9091\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "| 98.3471   | 92.562    | 91.7355   | 99.1736   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1.65289  |  7.43802  |  8.26446  |  0.826446 | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.942857 |  0.863636 |  0.931034 |  0.923077 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1        |  0.926829 |  0.771429 |  1        | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1        |  0.926829 |  0.771429 |  1        | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.977273 |  0.925    |  0.976744 |  0.990826 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.970588 |  0.894118 |  0.84375  |  0.96     | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.959911 |  0.893792 |  0.953615 |  0.956352 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.959234 |  0.842642 |  0.802443 |  0.955614 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "Soynet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 87ms/step - loss: 1.0856 - accuracy: 0.5620\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |    Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+============+===========+==================================+\n",
      "| 91.7355   | 67.7686   | 57.0248    | 95.8678   | Accuracy                         |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  8.26446  | 32.2314   | 42.9752    |  4.13223  | Error                            |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.780488 |  0.52     |  0.130435  |  1        | Precision                        |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.634146 |  0.0857143 |  0.583333 | Recall                           |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.634146 |  0.0857143 |  0.583333 | Sensitivity                      |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.897727 |  0.7      |  0.767442  |  1        | Specificity                      |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.864865 |  0.571429 |  0.103448  |  0.736842 | F1 Score                         |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.837057 |  0.603324 |  0.316388  |  1        | G Mean                           |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.816517 |  0.42405  |  0.080623  |  0.75172  | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "Xception Network Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 833ms/step - loss: 0.5795 - accuracy: 0.8512\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "| 96.6942   | 86.7769   | 88.4298   | 98.3471   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  3.30579  | 13.2231   | 11.5702   |  1.65289  | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.914286 |  0.777778 |  0.888889 |  0.857143 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.853659 |  0.685714 |  1        | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.853659 |  0.685714 |  1        | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.965909 |  0.875    |  0.965116 |  0.981651 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.941176 |  0.813953 |  0.774194 |  0.923077 | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.939743 |  0.824958 |  0.926219 |  0.917287 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.919323 |  0.730309 |  0.723604 |  0.915858 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "Densenet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 837ms/step - loss: 0.4038 - accuracy: 0.9008\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "| 99.1736   | 90.9091   | 90.9091   | 99.1736   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.826446 |  9.09091  |  9.09091  |  0.826446 | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.970588 |  0.840909 |  0.9      |  0.923077 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1        |  0.902439 |  0.771429 |  1        | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1        |  0.902439 |  0.771429 |  1        | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.988636 |  0.9125   |  0.965116 |  0.990826 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.985075 |  0.870588 |  0.830769 |  0.96     | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.979571 |  0.875973 |  0.93199  |  0.956352 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.97923  |  0.809344 |  0.783511 |  0.955614 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "Alexnet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 153ms/step - loss: 1.5966 - accuracy: 0.6942\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "| 97.5207   | 70.2479   | 72.7273   | 98.3471   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  2.47934  | 29.7521   | 27.2727   |  1.65289  | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.916667 |  0.555556 |  0.533333 |  1        | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1        |  0.609756 |  0.457143 |  0.833333 | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1        |  0.609756 |  0.457143 |  0.833333 | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.965909 |  0.75     |  0.837209 |  1        | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.956522 |  0.581395 |  0.492308 |  0.909091 | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.940966 |  0.645497 |  0.668215 |  1        | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.939959 |  0.446665 |  0.403616 |  0.906269 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "VGG-16 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 2s/step - loss: 1.1962 - accuracy: 0.4793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "| 85.9504   | 57.8512   | 61.9835   |  90.0826  | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "| 14.0496   | 42.1488   | 38.0165   |   9.91736 | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.690476 |  0.413793 |  0.238095 | nan       | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.878788 |  0.585366 |  0.142857 |   0       | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.878788 |  0.585366 |  0.142857 |   0       | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.852273 |  0.575    |  0.813953 |   1       | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.773333 |  0.484848 |  0.178571 | nan       | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.767121 |  0.487782 |  0.440225 | nan       | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.697786 |  0.313985 |  0.14478  | inf       | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "Resnet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 789ms/step - loss: 1.3043 - accuracy: 0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |     Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+=============+===========+===========+==================================+\n",
      "|   72.7273 |   33.8843   |   71.0744 |  90.0826  | Accuracy                         |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|   27.2727 |   66.1157   |   28.9256 |   9.91736 | Error                            |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0.338843 |  nan      | nan       | Precision                        |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    0      |    1        |    0      |   0       | Recall                           |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    0      |    1        |    0      |   0       | Sensitivity                      |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    1      |    0        |    1      |   1       | Specificity                      |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0.506173 |  nan      | nan       | F1 Score                         |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0        |  nan      | nan       | G Mean                           |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  inf      | -inf        |  inf      | inf       | Matthews Correlation Coefficient |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "EfficientNet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 579ms/step - loss: 1.3089 - accuracy: 0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |     Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+=============+===========+===========+==================================+\n",
      "|   72.7273 |   33.8843   |   71.0744 |  90.0826  | Accuracy                         |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|   27.2727 |   66.1157   |   28.9256 |   9.91736 | Error                            |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0.338843 |  nan      | nan       | Precision                        |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    0      |    1        |    0      |   0       | Recall                           |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    0      |    1        |    0      |   0       | Sensitivity                      |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    1      |    0        |    1      |   1       | Specificity                      |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0.506173 |  nan      | nan       | F1 Score                         |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0        |  nan      | nan       | G Mean                           |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  inf      | -inf        |  inf      | inf       | Matthews Correlation Coefficient |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "save_histogram_for_training_and_testing_accuracy(model_list,training_acc,testing_acc,file_path)\n",
      "save_of_training_accuracy_vs_epochs(model_history,epochs,file_path)\n",
      "save_of_training_loss_vs_epochs(model_history,epochs,file_path)\n",
      "print_evaluation_metrics(model,X_test_numpy_array,y_test,file_path)\n",
      "save_conf_matrix(model,X_test,y_test,class_list,file_path)\n",
      "save_histogram_for_training_and_testing_data(class_list,training_data_dict,testing_data_dict,file_path)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "import json,codecs\n",
    "\n",
    "def saveHist(path,history):\n",
    "    \n",
    "    new_hist = {}\n",
    "    for key in list(history.history.keys()):\n",
    "        new_hist[key]=history.history[key]\n",
    "        if type(history.history[key]) == np.ndarray:\n",
    "            new_hist[key] = history.history[key].tolist()\n",
    "        elif type(history.history[key]) == list:\n",
    "            if  type(history.history[key][0]) == np.float64:\n",
    "                new_hist[key] = list(map(float, history.history[key]))\n",
    "            \n",
    "    print(new_hist)\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(new_hist, file, separators=(',', ':'), sort_keys=True, indent=4) \n",
    "\n",
    "def loadHist(path):\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as file:\n",
    "        n = json.loads(file.read())\n",
    "    return n\n",
    "\n",
    "class_dict = {}\n",
    "\n",
    "counter = 0\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        class_dict[subfolder] = counter\n",
    "        counter += 1\n",
    "    break\n",
    "\n",
    "#print(counter)\n",
    "class_dict\n",
    "\n",
    "class_list = []\n",
    "for ele in class_dict:\n",
    "    class_list.append(ele)\n",
    "\n",
    "class_list\n",
    "\n",
    "import random\n",
    "\n",
    "def image_augmentation(img):\n",
    "    central_cropped_image = tf.image.central_crop(img_array, central_fraction=0.9)\n",
    "    random_left_right_flip = tf.image.stateless_random_flip_left_right(img,(2,3))\n",
    "    random_up_down_flip = tf.image.stateless_random_flip_up_down(img,(2,3))\n",
    "    random_contrast = tf.image.stateless_random_contrast(img,0.5, 0.9,(2,3))\n",
    "    random_saturation = tf.image.stateless_random_saturation(img,0.5, 0.9,(2,3))\n",
    "    random_brightness = tf.image.stateless_random_brightness(img,0.2,(2,3))\n",
    "        \n",
    "    imgs = [central_cropped_image, random_left_right_flip, random_up_down_flip, random_contrast, random_saturation, random_brightness]\n",
    "    return imgs\n",
    "\n",
    "\n",
    "IMG_SIZE_X = 200\n",
    "IMG_SIZE_Y = 200\n",
    "\n",
    "total_training_images_classwise = {}\n",
    "total_testing_images_classwise ={}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        path_to_subfolder = os.path.join(path,subfolder)\n",
    "        key = subfolder\n",
    "        number_of_image_in_folder = len(os.listdir(path_to_subfolder))\n",
    "        if folder == \"validation\":\n",
    "            total_testing_images_classwise[subfolder] = number_of_image_in_folder\n",
    "        else:\n",
    "            total_training_images_classwise[subfolder] = 6*number_of_image_in_folder\n",
    "        for img in os.listdir(path_to_subfolder):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path_to_subfolder,img))\n",
    "                \n",
    "                if folder == \"validation\":\n",
    "                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                    X_test.append(resized_array)\n",
    "                    y_test.append(class_dict[subfolder])\n",
    "                else:\n",
    "                    augmented_image = image_augmentation(img_array)\n",
    "                    for imgs in augmented_image:\n",
    "                      numpy_image = imgs.numpy().astype(\"uint8\")\n",
    "                      resized_array = cv2.resize(numpy_image, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                      X_train.append(resized_array)\n",
    "                      y_train.append(class_dict[subfolder])\n",
    "            except Exception as e:\n",
    "                print(\"Exception is : \"+e)\n",
    "                pass\n",
    "            \n",
    "print(\"Size of X_train : \" + str(len(X_train)))\n",
    "print(\"Size of y_train : \" + str(len(y_train)))\n",
    "print(\"Size of X_test : \" + str(len(X_test)))\n",
    "print(\"Size of y_test : \" + str(len(y_test)))\n",
    "\n",
    "file_path = path_to_save\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_data(class_list,total_training_images_classwise,total_testing_images_classwise,file_path+\"histogram_for_training_and_testing_data.jpg\")\n",
    "\n",
    "X_train_numpy_array = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test_numpy_array = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_numpy_array = X_train_numpy_array/255\n",
    "X_test_numpy_array = X_test_numpy_array/255\n",
    "\n",
    "input_shape = IMG_SIZE_X, IMG_SIZE_Y, 3\n",
    "n_classes = counter\n",
    "epoch = 50\n",
    "# Adagrad\n",
    "adagrad_opt = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.0005,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    name=\"Adagrad\",\n",
    ")\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "model_history = []\n",
    "\n",
    "model_name = \"proposed_model_1\"\n",
    "print(\"Proposed Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    proposed_model_1 = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    proposed_model_1.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    proposed_model_1_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    proposed_model_1 = model_import_file.proposed_model_1(input_shape,n_classes)\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    proposed_model_1_history = proposed_model_1.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = proposed_model_1.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    proposed_model_1.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",proposed_model_1_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "proposed_model_1_eval = proposed_model_1.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(proposed_model_1_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"proposed_model_1\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(proposed_model_1,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(proposed_model_1,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "# For deleting files if not wanted\n",
    "# model_name = \"proposed_model_1\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "    os.remove(path_to_save + model_name + '.json')\n",
    "    os.remove(path_to_save + model_name + '.h5')\n",
    "    os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_name = \"soyNet\"\n",
    "print(\"Soynet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    soyNet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    soyNet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    soyNet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    soyNet_model = model_import_file.soyNet(input_shape,n_classes)\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    soyNet_model_history = soyNet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = soyNet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    soyNet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",soyNet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "soyNet_model_eval = soyNet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "# For deleting files if not wanted\n",
    "model_name = \"soyNet\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "  os.remove(path_to_save + model_name + '.json')\n",
    "  os.remove(path_to_save + model_name + '.h5')\n",
    "  os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_history.append(soyNet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"soyNet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(soyNet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(soyNet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"xception_network\"\n",
    "print(\"Xception Network Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    xception_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    xception_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    xception_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    xception_model = model_import_file.xception_network(input_shape,n_classes)\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    xception_model_history = xception_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = xception_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    xception_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",xception_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "xception_model_eval = xception_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(xception_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"xception_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(xception_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(xception_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"densenet_121_network\"\n",
    "print(\"Densenet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    densenet_121_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    densenet_121_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    densenet_121_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    densenet_121_model = model_import_file.densenet_121_network(input_shape,n_classes)\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    densenet_121_model_history = densenet_121_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = densenet_121_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    densenet_121_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",densenet_121_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "densenet_121_model_eval = densenet_121_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(densenet_121_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"densenet_121_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(densenet_121_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(densenet_121_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"alexnet\"\n",
    "print(\"Alexnet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    alexnet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    alexnet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    alexnet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    alexnet_model = model_import_file.alexnet(input_shape,n_classes)\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    alexnet_model_history = alexnet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = alexnet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    alexnet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",alexnet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "alexnet_model_eval = alexnet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(alexnet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"alexnet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(alexnet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(alexnet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_VGG16\"\n",
    "print(\"VGG-16 Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_VGG16_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_VGG16_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_VGG16_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_VGG16_model = model_import_file.pretrained_VGG16(input_shape,n_classes)\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_VGG16_model_history = pretrained_VGG16_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_VGG16_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_VGG16_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_VGG16_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_VGG16_model_eval = pretrained_VGG16_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_VGG16_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_VGG16\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_VGG16_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_VGG16_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "print(\"Resnet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_ResNet50_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_ResNet50_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_ResNet50_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_ResNet50_model = model_import_file.pretrained_ResNet50(input_shape,n_classes)\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_ResNet50_model_history = pretrained_ResNet50_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_ResNet50_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_ResNet50_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_ResNet50_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_ResNet50_model_eval = pretrained_ResNet50_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_ResNet50_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_ResNet50_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_ResNet50_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "print(\"EfficientNet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_EfficientNetB2_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_EfficientNetB2_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_EfficientNetB2_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_EfficientNetB2_model = model_import_file.pretrained_EfficientNetB2(input_shape,n_classes)\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_EfficientNetB2_model_history = pretrained_EfficientNetB2_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_EfficientNetB2_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_EfficientNetB2_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_EfficientNetB2_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_EfficientNetB2_model_eval = pretrained_EfficientNetB2_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_EfficientNetB2_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "evaluation_metrics_import_file.save_of_training_accuracy_vs_epochs(model_history,epoch,file_path+\"training_accuracy_vs_epochs\")\n",
    "evaluation_metrics_import_file.save_of_training_loss_vs_epochs(model_history,epoch,file_path+\"training_loss_vs_epochs\")\n",
    "\n",
    "evaluation_metrics_import_file.func_help()\n",
    "\n",
    "training_accuracy_of_different_models = []\n",
    "testing_accuracy_of_different_models = []\n",
    "model_list = [\"Proposed Model 1\",\"SoyNet Model\",\"Xception Model\",\"Densenet 121 Model\",\"AlexNet Model\",\"VGG16 Model\",\"ResNet 50 Model\",\"EfficientNetB2 Model\"]\n",
    "\n",
    "sz = len(model_history)\n",
    "for i in range(sz):\n",
    "    length = len(model_history[i]['accuracy'])\n",
    "    training_accuracy_of_different_models.append(model_history[i]['accuracy'][length-1])\n",
    "\n",
    "\n",
    "testing_accuracy_of_different_models.append(proposed_model_1_eval[1])\n",
    "testing_accuracy_of_different_models.append(soyNet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(xception_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(densenet_121_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(alexnet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_VGG16_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_ResNet50_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_EfficientNetB2_model_eval[1])\n",
    "\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_accuracy(model_list,training_accuracy_of_different_models,testing_accuracy_of_different_models,file_path+\"training_and_testing_accuracy.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49921973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04078a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6ca0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48996313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65a1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
