{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb3164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:22:18.193562: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:18.193603: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Citrus_data_report/import file')\n",
    "\n",
    "import model_import_file\n",
    "import evaluation_metrics_import_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e038ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"dataset\"\n",
    "path_to_save = \"Citrus_data_report/data_save_without_aug/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c11d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 23})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbda3a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['canker', 'greening', 'blackspot', 'healthy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "import json,codecs\n",
    "\n",
    "def saveHist(path,history):\n",
    "    \n",
    "    new_hist = {}\n",
    "    for key in list(history.history.keys()):\n",
    "        new_hist[key]=history.history[key]\n",
    "        if type(history.history[key]) == np.ndarray:\n",
    "            new_hist[key] = history.history[key].tolist()\n",
    "        elif type(history.history[key]) == list:\n",
    "            if  type(history.history[key][0]) == np.float64:\n",
    "                new_hist[key] = list(map(float, history.history[key]))\n",
    "            \n",
    "    print(new_hist)\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(new_hist, file, separators=(',', ':'), sort_keys=True, indent=4) \n",
    "\n",
    "def loadHist(path):\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as file:\n",
    "        n = json.loads(file.read())\n",
    "    return n\n",
    "\n",
    "class_dict = {}\n",
    "\n",
    "counter = 0\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        class_dict[subfolder] = counter\n",
    "        counter += 1\n",
    "    break\n",
    "\n",
    "print(counter)\n",
    "class_dict\n",
    "\n",
    "class_list = []\n",
    "for ele in class_dict:\n",
    "    class_list.append(ele)\n",
    "\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8812661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train : 475\n",
      "Size of y_train : 475\n",
      "Size of X_test : 121\n",
      "Size of y_test : 121\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE_X = 200\n",
    "IMG_SIZE_Y = 200\n",
    "\n",
    "total_training_images_classwise = {}\n",
    "total_testing_images_classwise ={}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        path_to_subfolder = os.path.join(path,subfolder)\n",
    "        key = subfolder\n",
    "        number_of_image_in_folder = len(os.listdir(path_to_subfolder))\n",
    "        if folder == \"validation\":\n",
    "            total_testing_images_classwise[subfolder] = number_of_image_in_folder\n",
    "        else:\n",
    "            total_training_images_classwise[subfolder] = number_of_image_in_folder\n",
    "        for img in os.listdir(path_to_subfolder):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path_to_subfolder,img))\n",
    "                if folder == \"validation\":\n",
    "                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                    X_test.append(resized_array)\n",
    "                    y_test.append(class_dict[subfolder])\n",
    "                else:\n",
    "                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                    X_train.append(resized_array)\n",
    "                    y_train.append(class_dict[subfolder])\n",
    "            except Exception as e:\n",
    "                print(\"Exception is : \"+e)\n",
    "                pass\n",
    "            \n",
    "print(\"Size of X_train : \" + str(len(X_train)))\n",
    "print(\"Size of y_train : \" + str(len(y_train)))\n",
    "print(\"Size of X_test : \" + str(len(X_test)))\n",
    "print(\"Size of y_test : \" + str(len(y_test)))\n",
    "\n",
    "file_path = path_to_save\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_data(class_list,total_training_images_classwise,total_testing_images_classwise,file_path+\"histogram_for_training_and_testing_data.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264189b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:22:24.114458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:24.114655: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:24.114772: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:24.114872: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:24.115006: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:24.115099: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:24.115187: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:24.115276: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:22:24.115294: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-01 13:22:24.115809: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 8s 784ms/step - loss: 0.2999 - accuracy: 0.9174\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "|       100 | 93.3884   | 93.3884   | 96.6942   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|         0 |  6.61157  |  6.61157  |  3.30579  | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|         1 |  0.866667 |  1        |  0.75     | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|         1 |  0.95122  |  0.771429 |  1        | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|         1 |  0.95122  |  0.771429 |  1        | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|         1 |  0.925    |  1        |  0.963303 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|         1 |  0.906977 |  0.870968 |  0.857143 | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|         1 |  0.895358 |  1        |  0.849987 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|         1 |  0.860486 |  0.842999 |  0.847288 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "Soynet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 92ms/step - loss: 1.2038 - accuracy: 0.4298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |    Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+============+===========+==================================+\n",
      "| 74.3802   | 59.5041   | 61.9835    |  90.0826  | Accuracy                         |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "| 25.6198   | 40.4959   | 38.0165    |   9.91736 | Error                            |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.52     |  0.431034 |  0.0769231 | nan       | Precision                        |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.787879 |  0.609756 |  0.0285714 |   0       | Recall                           |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.787879 |  0.609756 |  0.0285714 |   0       | Sensitivity                      |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.727273 |  0.5875   |  0.860465  |   1       | Specificity                      |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.626506 |  0.505051 |  0.0416667 | nan       | F1 Score                         |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.614965 |  0.503222 |  0.257273  | nan       | G Mean                           |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "|  0.512957 |  0.334493 |  0.0466987 | inf       | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+------------+-----------+----------------------------------+\n",
      "Xception Network Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 751ms/step - loss: 0.5590 - accuracy: 0.8760\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "| 98.3471   | 88.4298   | 89.2562   | 99.1736   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1.65289  | 11.5702   | 10.7438   |  0.826446 | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.77551  |  0.923077 |  0.923077 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.926829 |  0.685714 |  1        | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.926829 |  0.685714 |  1        | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.988636 |  0.8625   |  0.976744 |  0.990826 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.844444 |  0.786885 |  0.96     | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.979121 |  0.817849 |  0.949531 |  0.956352 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.958678 |  0.76843  |  0.742666 |  0.955614 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "DenseNet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 864ms/step - loss: 0.4189 - accuracy: 0.8678\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "| 98.3471   | 86.7769   | 88.4298   |       100 | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1.65289  | 13.2231   | 11.5702   |         0 | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.942857 |  0.878788 |  0.756098 |         1 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1        |  0.707317 |  0.885714 |         1 | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  1        |  0.707317 |  0.885714 |         1 | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.977273 |  0.95     |  0.883721 |         1 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.970588 |  0.783784 |  0.815789 |         1 | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.959911 |  0.9137   |  0.817422 |         1 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.959234 |  0.716721 |  0.747907 |         1 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "Alexnet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 212ms/step - loss: 1.5601 - accuracy: 0.5950\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+===========+===========+==================================+\n",
      "| 93.3884   | 63.6364   | 67.7686   | 94.2149   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  6.61157  | 36.3636   | 32.2314   |  5.78512  | Error                            |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.820513 |  0.469388 |  0.409091 |  0.727273 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.560976 |  0.257143 |  0.666667 | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.560976 |  0.257143 |  0.666667 | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.920455 |  0.675    |  0.848837 |  0.972477 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.888889 |  0.511111 |  0.315789 |  0.695652 | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.869048 |  0.562883 |  0.589281 |  0.840985 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "|  0.848576 |  0.362755 |  0.261675 |  0.674856 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+----------------------------------+\n",
      "VGG-16 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 2s/step - loss: 1.2472 - accuracy: 0.4380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:100: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f1_sc = (2*prec*rec) / (prec + rec)\n",
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |     Class 3 |   Class 4 | Measure                          |\n",
      "+===========+===========+=============+===========+==================================+\n",
      "| 78.5124   | 52.0661   |  66.9421    |  90.0826  | Accuracy                         |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "| 21.4876   | 47.9339   |  33.0579    |   9.91736 | Error                            |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "|  0.62069  |  0.402299 |   0         | nan       | Precision                        |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "|  0.545455 |  0.853659 |   0         |   0       | Recall                           |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "|  0.545455 |  0.853659 |   0         |   0       | Sensitivity                      |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "|  0.875    |  0.35     |   0.94186   |   1       | Specificity                      |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "|  0.580645 |  0.546875 | nan         | nan       | F1 Score                         |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "|  0.736956 |  0.375239 |   0         | nan       | G Mean                           |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "|  0.499371 |  0.299855 |   0.0227051 | inf       | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-------------+-----------+----------------------------------+\n",
      "Resnet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 793ms/step - loss: 1.3071 - accuracy: 0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |     Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+=============+===========+===========+==================================+\n",
      "|   72.7273 |   33.8843   |   71.0744 |  90.0826  | Accuracy                         |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|   27.2727 |   66.1157   |   28.9256 |   9.91736 | Error                            |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0.338843 |  nan      | nan       | Precision                        |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    0      |    1        |    0      |   0       | Recall                           |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    0      |    1        |    0      |   0       | Sensitivity                      |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    1      |    0        |    1      |   1       | Specificity                      |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0.506173 |  nan      | nan       | F1 Score                         |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0        |  nan      | nan       | G Mean                           |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  inf      | -inf        |  inf      | inf       | Matthews Correlation Coefficient |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "EfficientNet B2 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 586ms/step - loss: 1.3090 - accuracy: 0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "Citrus_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |     Class 2 |   Class 3 |   Class 4 | Measure                          |\n",
      "+===========+=============+===========+===========+==================================+\n",
      "|   72.7273 |   33.8843   |   71.0744 |  90.0826  | Accuracy                         |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|   27.2727 |   66.1157   |   28.9256 |   9.91736 | Error                            |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0.338843 |  nan      | nan       | Precision                        |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    0      |    1        |    0      |   0       | Recall                           |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    0      |    1        |    0      |   0       | Sensitivity                      |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|    1      |    0        |    1      |   1       | Specificity                      |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0.506173 |  nan      | nan       | F1 Score                         |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  nan      |    0        |  nan      | nan       | G Mean                           |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "|  inf      | -inf        |  inf      | inf       | Matthews Correlation Coefficient |\n",
      "+-----------+-------------+-----------+-----------+----------------------------------+\n",
      "save_histogram_for_training_and_testing_accuracy(model_list,training_acc,testing_acc,file_path)\n",
      "save_of_training_accuracy_vs_epochs(model_history,epochs,file_path)\n",
      "save_of_training_loss_vs_epochs(model_history,epochs,file_path)\n",
      "print_evaluation_metrics(model,X_test_numpy_array,y_test,file_path)\n",
      "save_conf_matrix(model,X_test,y_test,class_list,file_path)\n",
      "save_histogram_for_training_and_testing_data(class_list,training_data_dict,testing_data_dict,file_path)\n"
     ]
    }
   ],
   "source": [
    "X_train_numpy_array = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test_numpy_array = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_numpy_array = X_train_numpy_array/255\n",
    "X_test_numpy_array = X_test_numpy_array/255\n",
    "\n",
    "input_shape = IMG_SIZE_X, IMG_SIZE_Y, 3\n",
    "n_classes = counter\n",
    "epoch = 50\n",
    "# Adagrad\n",
    "adagrad_opt = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.0005,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    name=\"Adagrad\",\n",
    ")\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "model_history = []\n",
    "\n",
    "model_name = \"proposed_model_1\"\n",
    "print(\"Proposed Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    proposed_model_1 = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    proposed_model_1.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    proposed_model_1_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    proposed_model_1 = model_import_file.proposed_model_1(input_shape,n_classes)\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    proposed_model_1_history = proposed_model_1.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = proposed_model_1.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    proposed_model_1.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",proposed_model_1_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "proposed_model_1_eval = proposed_model_1.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(proposed_model_1_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"proposed_model_1\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(proposed_model_1,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(proposed_model_1,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "# For deleting files if not wanted\n",
    "# model_name = \"proposed_model_1\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "    os.remove(path_to_save + model_name + '.json')\n",
    "    os.remove(path_to_save + model_name + '.h5')\n",
    "    os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_name = \"soyNet\"\n",
    "print(\"Soynet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    soyNet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    soyNet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    soyNet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    soyNet_model = model_import_file.soyNet(input_shape,n_classes)\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    soyNet_model_history = soyNet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = soyNet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    soyNet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",soyNet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "soyNet_model_eval = soyNet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "# For deleting files if not wanted\n",
    "model_name = \"soyNet\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "  os.remove(path_to_save + model_name + '.json')\n",
    "  os.remove(path_to_save + model_name + '.h5')\n",
    "  os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_history.append(soyNet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"soyNet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(soyNet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(soyNet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"xception_network\"\n",
    "print(\"Xception Network Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    xception_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    xception_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    xception_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    xception_model = model_import_file.xception_network(input_shape,n_classes)\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    xception_model_history = xception_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = xception_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    xception_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",xception_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "xception_model_eval = xception_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(xception_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"xception_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(xception_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(xception_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"densenet_121_network\"\n",
    "print(\"DenseNet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    densenet_121_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    densenet_121_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    densenet_121_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    densenet_121_model = model_import_file.densenet_121_network(input_shape,n_classes)\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    densenet_121_model_history = densenet_121_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = densenet_121_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    densenet_121_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",densenet_121_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "densenet_121_model_eval = densenet_121_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(densenet_121_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"densenet_121_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(densenet_121_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(densenet_121_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"alexnet\"\n",
    "print(\"Alexnet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    alexnet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    alexnet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    alexnet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    alexnet_model = model_import_file.alexnet(input_shape,n_classes)\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    alexnet_model_history = alexnet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = alexnet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    alexnet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",alexnet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "alexnet_model_eval = alexnet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(alexnet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"alexnet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(alexnet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(alexnet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_VGG16\"\n",
    "print(\"VGG-16 Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_VGG16_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_VGG16_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_VGG16_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_VGG16_model = model_import_file.pretrained_VGG16(input_shape,n_classes)\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_VGG16_model_history = pretrained_VGG16_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_VGG16_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_VGG16_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_VGG16_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_VGG16_model_eval = pretrained_VGG16_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_VGG16_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_VGG16\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_VGG16_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_VGG16_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "print(\"Resnet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_ResNet50_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_ResNet50_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_ResNet50_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_ResNet50_model = model_import_file.pretrained_ResNet50(input_shape,n_classes)\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_ResNet50_model_history = pretrained_ResNet50_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_ResNet50_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_ResNet50_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_ResNet50_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_ResNet50_model_eval = pretrained_ResNet50_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_ResNet50_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_ResNet50_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_ResNet50_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "print(\"EfficientNet B2 Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_EfficientNetB2_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_EfficientNetB2_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_EfficientNetB2_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_EfficientNetB2_model = model_import_file.pretrained_EfficientNetB2(input_shape,n_classes)\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_EfficientNetB2_model_history = pretrained_EfficientNetB2_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_EfficientNetB2_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_EfficientNetB2_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_EfficientNetB2_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_EfficientNetB2_model_eval = pretrained_EfficientNetB2_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_EfficientNetB2_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "evaluation_metrics_import_file.save_of_training_accuracy_vs_epochs(model_history,epoch,file_path+\"training_accuracy_vs_epochs\")\n",
    "evaluation_metrics_import_file.save_of_training_loss_vs_epochs(model_history,epoch,file_path+\"training_loss_vs_epochs\")\n",
    "\n",
    "evaluation_metrics_import_file.func_help()\n",
    "\n",
    "training_accuracy_of_different_models = []\n",
    "testing_accuracy_of_different_models = []\n",
    "model_list = [\"Proposed Model 1\",\"SoyNet Model\",\"Xception Model\",\"Densenet 121 Model\",\"AlexNet Model\",\"VGG16 Model\",\"ResNet 50 Model\",\"EfficientNetB2 Model\"]\n",
    "\n",
    "sz = len(model_history)\n",
    "for i in range(sz):\n",
    "    length = len(model_history[i]['accuracy'])\n",
    "    training_accuracy_of_different_models.append(model_history[i]['accuracy'][length-1])\n",
    "\n",
    "\n",
    "testing_accuracy_of_different_models.append(proposed_model_1_eval[1])\n",
    "testing_accuracy_of_different_models.append(soyNet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(xception_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(densenet_121_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(alexnet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_VGG16_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_ResNet50_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_EfficientNetB2_model_eval[1])\n",
    "\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_accuracy(model_list,training_accuracy_of_different_models,testing_accuracy_of_different_models,file_path+\"training_and_testing_accuracy.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917c1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71831dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
