# -*- coding: utf-8 -*-
"""model-import-file.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lbg27aalv3_u25XpnWuYbXt4T5uqvwO4
"""

import tensorflow
import tensorflow as tf
import keras
import keras.backend as K
from keras.models import Model
from keras.layers import Input, Dense, Conv2D, Conv3D, DepthwiseConv2D, SeparableConv2D, Conv3DTranspose
from keras.layers import Flatten, MaxPool2D, AvgPool2D, GlobalAvgPool2D, UpSampling2D, BatchNormalization
from keras.layers import Concatenate, Add, Dropout, ReLU, Lambda, Activation, LeakyReLU, PReLU


from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

import time
import numpy as np

def proposed_model_1(input_shape,output_shape):
    def bn_rl_conv(x, filters, kernel_size):
        x = BatchNormalization()(x)
        x = ReLU()(x)
        if kernel_size == 1:
            x = Conv2D(filters=filters,
                    kernel_size=kernel_size,
                    padding='same')(x)
        else:
            x = Conv2D(filters=filters,
                    kernel_size=(1,kernel_size),
                    padding='same')(x)
            x = Conv2D(filters=filters,
                    kernel_size=(kernel_size,1),
                    padding='same')(x)
        return x

    def collective_block(tensor, k, reps):
        feature_list = [tensor]
        for _ in range(reps):
            x = bn_rl_conv(tensor, filters=4*k, kernel_size=1)
            x = bn_rl_conv(x, filters=k, kernel_size=3)
#             tensor = Concatenate()([tensor, x])
            feature_list.append(x)
            tensor = Concatenate()(feature_list)
            
        return tensor

    def passage_layer(x, theta):
        f = int(tensorflow.keras.backend.int_shape(x)[-1] * theta)
        x = bn_rl_conv(x, filters=f, kernel_size=1)
        x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)
        return x

    k = 32
    theta = 0.5
    repetitions = 6, 12, 24, 16
    
    input = Input(input_shape)
    
    x = Conv2D(2*k, 7, strides=2, padding='same')(input)
    x = MaxPool2D(3, strides=2, padding='same')(x)
    
    passage_layer_output = []
    for reps in repetitions:
        d = collective_block(x, k, reps)
        x = passage_layer(d, theta)
        length = len(passage_layer_output)
        if length > 0:
            ele = passage_layer_output[length-1]
            extr = passage_layer(ele, theta)
            x = Concatenate()([x, extr])
        passage_layer_output.append(x)
    
    x = GlobalAvgPool2D()(d)
    # Actual Activation function is softmax
    output = Dense(output_shape, activation='softmax')(x)
    model = Model(input, output)

    return model

def proposed_model_2(input_shape,output_shape):
    def bn_rl_conv(x, filters, kernel_size):
        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = Conv2D(filters=filters,
                kernel_size=kernel_size,
                padding='same')(x)
        return x

    def dense_block(tensor, k, reps):
        for _ in range(reps):
            x = bn_rl_conv(tensor, filters=4*k, kernel_size=1)
            x = bn_rl_conv(x, filters=k, kernel_size=3)
            tensor = Concatenate()([tensor, x])
        return tensor

    def transition_layer(x, theta):
        f = int(tensorflow.keras.backend.int_shape(x)[-1] * theta)
        x = bn_rl_conv(x, filters=f, kernel_size=1)
        x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)
        return x

    k = 32
    theta = 0.5
    repetitions = 6, 12, 24, 16
    
    input = Input(input_shape)
    
    x = Conv2D(2*k, 7, strides=2, padding='same')(input)
    x = MaxPool2D(3, strides=2, padding='same')(x)
    
    transition_layer_output = []
    for reps in repetitions:
        d = dense_block(x, k, reps)
        x = transition_layer(d, theta)
        length = len(transition_layer_output)
        if length > 0:
            ele = transition_layer_output[length-1]
            extr = transition_layer(ele, theta)
            x = Concatenate()([x, extr])
        transition_layer_output.append(x)
    
    x = GlobalAvgPool2D()(d)
    # Actual Activation function is softmax
    output = Dense(output_shape, activation='softmax')(x)
    model = Model(input, output)

    return model

def soyNet(input_shape,n_classes):
    input = Input(input_shape)
    
    x = Conv2D(32, 3, padding='same', activation='relu')(input)
    x = MaxPool2D(2, strides=2, padding='same')(x)
    
    x = Conv2D(32, 3, padding='same', activation='relu')(x)
    x = MaxPool2D(2, strides=2, padding='same')(x)
    
    x = Conv2D(64, 3, padding='same', activation='relu')(x)
    x = MaxPool2D(2, strides=2, padding='same')(x)
    
    x = Conv2D(64, 3, padding='same', activation='relu')(x)
    x = MaxPool2D(2, strides=2, padding='same')(x)
    
    x = Conv2D(128, 3, padding='same', activation='relu')(x)
    x = MaxPool2D(2, strides=2, padding='same')(x)
    
    x = Conv2D(128, 3, padding='same', activation='relu')(x)
    x = MaxPool2D(2, strides=2, padding='same')(x)
    
    x = Flatten()(x)
    
    x = Dense(512, activation='relu')(x)
    output = Dense(n_classes, activation='softmax')(x)
    
    model = Model(input, output)
    return model

def xception_network(input_shape,output_shape):
    def conv_bn(x, filters, kernel_size, strides=1):
        x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', use_bias=False)(x)
        x = BatchNormalization()(x)
        return x
    
    def sep_bn(x, filters, kernel_size, strides=1):
        x = SeparableConv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', use_bias=False)(x)
        x = BatchNormalization()(x)
        return x
    
    def entry_flow(x):
        x = conv_bn(x, filters=32, kernel_size=3, strides=2)
        x = ReLU()(x)
        x = conv_bn(x, filters=64, kernel_size=3)
        tensor = ReLU()(x)
    
        x = sep_bn(tensor, filters=128, kernel_size=3)
        x = ReLU()(x)
        x = sep_bn(x, filters=128, kernel_size=3)
        x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)
        tensor = conv_bn(tensor, filters=128, kernel_size=1, strides=2)
    
        x = Add()([tensor, x])
        x = ReLU()(x)
        x = sep_bn(x, filters=256, kernel_size=3)
        x = ReLU()(x)
        x = sep_bn(x, filters=256, kernel_size=3)
        x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)
        tensor = conv_bn(tensor, filters=256, kernel_size=1, strides=2)
    
        x = Add()([tensor, x])
        x = ReLU()(x)
        x = sep_bn(x, filters=728, kernel_size=3)
        x = ReLU()(x)
        x = sep_bn(x, filters=728, kernel_size=3)
        x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)
        tensor = conv_bn(tensor, filters=728, kernel_size=1, strides=2)
        x = Add()([tensor, x])
        return x
    
    def middle_flow(tensor):
        for _ in range(8):
            x = ReLU()(tensor)
            x = sep_bn(x, filters=728, kernel_size=3)
            x = ReLU()(x)
            x = sep_bn(x, filters=728, kernel_size=3)
            x = ReLU()(x)
            x = sep_bn(x, filters=728, kernel_size=3)
            tensor = Add()([tensor, x])
        return tensor

    def exit_flow(tensor):
        x = ReLU()(tensor)
        x = sep_bn(x, filters=728, kernel_size=3)
        x = ReLU()(x)
        x = sep_bn(x, filters=1024, kernel_size=3)
        x = MaxPool2D(3, strides=2, padding='same')(x)

        tensor = conv_bn(tensor, filters=1024, kernel_size=1, strides=2)
        x = Add()([tensor, x])
        x = sep_bn(x, filters=1536, kernel_size=3)
        x = ReLU()(x)
        x = sep_bn(x, filters=2048, kernel_size=3)
        x = ReLU()(x)
        x = GlobalAvgPool2D()(x)
        # Match it with number of output
        # Actual Activation function is softmax
        x = Dense(output_shape, activation='sigmoid')(x)
        return x

    input = Input(input_shape)
    x = entry_flow(input)
    x = middle_flow(x)
    output = exit_flow(x)
    model = Model(input, output)
    return model

def densenet_121_network(input_shape,output_shape):
    def bn_rl_conv(x, filters, kernel_size):
        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = Conv2D(filters=filters,
                kernel_size=kernel_size,
                padding='same')(x)
        return x

    def dense_block(tensor, k, reps):
        feature_list = [tensor]
        for _ in range(reps):
            x = bn_rl_conv(tensor, filters=4*k, kernel_size=1)
            x = bn_rl_conv(x, filters=k, kernel_size=3)
#             tensor = Concatenate()([tensor, x])
            feature_list.append(x)
            tensor = Concatenate()(feature_list)
            
        return tensor

    def transition_layer(x, theta):
        f = int(tensorflow.keras.backend.int_shape(x)[-1] * theta)
        x = bn_rl_conv(x, filters=f, kernel_size=1)
        x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)
        return x

    k = 32
    theta = 0.5
    repetitions = 6, 12, 24, 16
    
    input = Input(input_shape)
    
    x = Conv2D(2*k, 7, strides=2, padding='same')(input)
    x = MaxPool2D(3, strides=2, padding='same')(x)
    
    for reps in repetitions:
        d = dense_block(x, k, reps)
        x = transition_layer(d, theta)
    
    x = GlobalAvgPool2D()(d)
    # Actual Activation function is softmax
    output = Dense(output_shape, activation='softmax')(x)
    model = Model(input, output)

    return model

def densenet_169_network(input_shape,output_shape):
    def bn_rl_conv(x, filters, kernel_size):
        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = Conv2D(filters=filters,
                kernel_size=kernel_size,
                padding='same')(x)
        return x

    def dense_block(tensor, k, reps):
        for _ in range(reps):
            x = bn_rl_conv(tensor, filters=4*k, kernel_size=1)
            x = bn_rl_conv(x, filters=k, kernel_size=3)
            tensor = Concatenate()([tensor, x])
        return tensor

    def transition_layer(x, theta):
        f = int(tensorflow.keras.backend.int_shape(x)[-1] * theta)
        x = bn_rl_conv(x, filters=f, kernel_size=1)
        x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)
        return x

    k = 32
    theta = 0.5
    repetitions = 6, 12, 32, 32
    
    input = Input(input_shape)
    
    x = Conv2D(2*k, 7, strides=2, padding='same')(input)
    x = MaxPool2D(3, strides=2, padding='same')(x)
    
    for reps in repetitions:
        d = dense_block(x, k, reps)
        x = transition_layer(d, theta)
    
    x = GlobalAvgPool2D()(d)
    # Actual Activation function is softmax
    output = Dense(output_shape, activation='softmax')(x)
    model = Model(input, output)

    return model

def densenet_201_network(input_shape,output_shape):
    def bn_rl_conv(x, filters, kernel_size):
        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = Conv2D(filters=filters,
                kernel_size=kernel_size,
                padding='same')(x)
        return x

    def dense_block(tensor, k, reps):
        for _ in range(reps):
            x = bn_rl_conv(tensor, filters=4*k, kernel_size=1)
            x = bn_rl_conv(x, filters=k, kernel_size=3)
            tensor = Concatenate()([tensor, x])
        return tensor

    def transition_layer(x, theta):
        f = int(tensorflow.keras.backend.int_shape(x)[-1] * theta)
        x = bn_rl_conv(x, filters=f, kernel_size=1)
        x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)
        return x

    k = 32
    theta = 0.5
    theta = 0.5
    repetitions = 6, 12, 48, 32
    
    input = Input(input_shape)
    
    x = Conv2D(2*k, 7, strides=2, padding='same')(input)
    x = MaxPool2D(3, strides=2, padding='same')(x)
    
    for reps in repetitions:
        d = dense_block(x, k, reps)
        x = transition_layer(d, theta)
    
    x = GlobalAvgPool2D()(d)
    # Actual Activation function is softmax
    output = Dense(output_shape, activation='softmax')(x)
    model = Model(input, output)

    return model

# Creating VGG-16 Model

def vgg_16(input_shape,output_shape):
    model = Sequential()

    model.add(Conv2D(input_shape=input_shape,filters=64,kernel_size=(3,3),padding="same", activation="relu"))
    model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Flatten())
    model.add(Dense(units=4096,activation="relu"))
    model.add(Dense(units=4096,activation="relu"))
    # Actual activation function is softmax
    model.add(Dense(output_shape, activation="sigmoid"))

    return model

def vgg_19(input_shape,output_shape):
    model = Sequential()

    model.add(Conv2D(input_shape=input_shape,filters=64,kernel_size=(3,3),padding="same", activation="relu"))
    model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    model.add(Flatten())
    model.add(Dense(units=4096,activation="relu"))
    model.add(Dense(units=4096,activation="relu"))
    # Actual one is softmax
    model.add(Dense(output_shape, activation="sigmoid"))

    return model

def resnet_50(input_shape, output_shape):
  
  def conv_bn_rl(x, f, k=1, s=1, p='same'):
    x = Conv2D(f, k, strides=s, padding=p)(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    return x
  
  
  def identity_block(tensor, f):
    x = conv_bn_rl(tensor, f)
    x = conv_bn_rl(x, f, 3)
    x = Conv2D(4*f, 1)(x)
    x = BatchNormalization()(x)
    
    x = Add()([x, tensor])
    output = ReLU()(x)
    return output
  
  
  def conv_block(tensor, f, s):
    x = conv_bn_rl(tensor, f)
    x = conv_bn_rl(x, f, 3, s)
    x = Conv2D(4*f, 1)(x)
    x = BatchNormalization()(x)
    
    shortcut = Conv2D(4*f, 1, strides=s)(tensor)
    shortcut = BatchNormalization()(shortcut)
    
    x = Add()([x, shortcut])
    output = ReLU()(x)
    return output
  
  
  def resnet_block(x, f, r, s=2):
    x = conv_block(x, f, s)
    for _ in range(r-1):
        x = identity_block(x, f)
    return x
    
  
    input = Input(input_shape)
  
    x = conv_bn_rl(input, 64, 7, 2)
    x = MaxPool2D(3, strides=2, padding='same')(x)

    x = resnet_block(x, 64, 3, 1)
    x = resnet_block(x, 128, 4)
    x = resnet_block(x, 256, 6)
    x = resnet_block(x, 512, 3)

    x = GlobalAvgPool2D()(x)

    output = Dense(output_shape, activation='softmax')(x)

    model = Model(input, output)
    return model

def resnet_101(input_shape, output_shape):
  
  def conv_bn_rl(x, f, k=1, s=1, p='same'):
    x = Conv2D(f, k, strides=s, padding=p)(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    return x
  
  
  def identity_block(tensor, f):
    x = conv_bn_rl(tensor, f)
    x = conv_bn_rl(x, f, 3)
    x = Conv2D(4*f, 1)(x)
    x = BatchNormalization()(x)
    
    x = Add()([x, tensor])
    output = ReLU()(x)
    return output
  
  
  def conv_block(tensor, f, s):
    x = conv_bn_rl(tensor, f)
    x = conv_bn_rl(x, f, 3, s)
    x = Conv2D(4*f, 1)(x)
    x = BatchNormalization()(x)
    
    shortcut = Conv2D(4*f, 1, strides=s)(tensor)
    shortcut = BatchNormalization()(shortcut)
    
    x = Add()([x, shortcut])
    output = ReLU()(x)
    return output
  
  
  def resnet_block(x, f, r, s=2):
    x = conv_block(x, f, s)
    for _ in range(r-1):
        x = identity_block(x, f)
    return x
    
  
    input = Input(input_shape)

    x = conv_bn_rl(input, 64, 7, 2)
    x = MaxPool2D(3, strides=2, padding='same')(x)

    x = resnet_block(x, 64, 3, 1)
    x = resnet_block(x, 128, 4)
    x = resnet_block(x, 256, 23)
    x = resnet_block(x, 512, 3)

    x = GlobalAvgPool2D()(x)

    output = Dense(output_shape, activation='softmax')(x)

    model = Model(input, output)
    return model

def resnet_152(input_shape, output_shape):
  
  def conv_bn_rl(x, f, k=1, s=1, p='same'):
    x = Conv2D(f, k, strides=s, padding=p)(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    return x
  
  
  def identity_block(tensor, f):
    x = conv_bn_rl(tensor, f)
    x = conv_bn_rl(x, f, 3)
    x = Conv2D(4*f, 1)(x)
    x = BatchNormalization()(x)
    
    x = Add()([x, tensor])
    output = ReLU()(x)
    return output
  
  
  def conv_block(tensor, f, s):
    x = conv_bn_rl(tensor, f)
    x = conv_bn_rl(x, f, 3, s)
    x = Conv2D(4*f, 1)(x)
    x = BatchNormalization()(x)
    
    shortcut = Conv2D(4*f, 1, strides=s)(tensor)
    shortcut = BatchNormalization()(shortcut)
    
    x = Add()([x, shortcut])
    output = ReLU()(x)
    return output
  
  
  def resnet_block(x, f, r, s=2):
    x = conv_block(x, f, s)
    for _ in range(r-1):
        x = identity_block(x, f)
    return x
    

    input = Input(input_shape)

    x = conv_bn_rl(input, 64, 7, 2)
    x = MaxPool2D(3, strides=2, padding='same')(x)

    x = resnet_block(x, 64, 3, 1)
    x = resnet_block(x, 128, 8)
    x = resnet_block(x, 256, 36)
    x = resnet_block(x, 512, 3)

    x = GlobalAvgPool2D()(x)

    output = Dense(output_shape, activation='softmax')(x)

    model = Model(input, output)
    return model

def squeezenet(input_shape, output_shape):
  
  def fire(x, fs, fe):
    s = Conv2D(fs, 1, activation='relu')(x)
    e1 = Conv2D(fe, 1, activation='relu')(s)
    e3 = Conv2D(fe, 3, padding='same', activation='relu')(s)
    output = Concatenate()([e1, e3])
    return output
  
  
    input = Input(input_shape)

    x = Conv2D(96, 7, strides=2, padding='same', activation='relu')(input)
    x = MaxPool2D(3, strides=2, padding='same')(x)

    x = fire(x, 16, 64)
    x = fire(x, 16, 64)
    x = fire(x, 32, 128)
    x = MaxPool2D(3, strides=2, padding='same')(x)

    x = fire(x, 32, 128)
    x = fire(x, 48, 192)
    x = fire(x, 48, 192)
    x = fire(x, 64, 256)
    x = MaxPool2D(3, strides=2, padding='same')(x)

    x = fire(x, 64, 256)
    x = Conv2D(output_shape, 1)(x)
    x = GlobalAvgPool2D()(x)

    output = Activation('softmax')(x)

    model = Model(input, output)
    return model

def alexnet(input_shape, output_shape):
    input = Input(input_shape)

    # actually batch normalization didn't exist back then
    # they used LRN (Local Response Normalization) for regularization
    x = Conv2D(96, 11, strides=4, padding='same', activation='relu')(input)
    x = BatchNormalization()(x)
    x = MaxPool2D(3, strides=2)(x)

    x = Conv2D(256, 5, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPool2D(3, strides=2)(x)

    x = Conv2D(384, 3, strides=1, padding='same', activation='relu')(x)

    x = Conv2D(384, 3, strides=1, padding='same', activation='relu')(x)

    x = Conv2D(256, 3, strides=1, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPool2D(3, strides=2)(x)

    x = Flatten()(x)
    x = Dense(4096, activation='relu')(x)
    x = Dense(4096, activation='relu')(x)

    output = Dense(output_shape, activation='softmax')(x)

    model = Model(input, output)
    return model

def pretrained_Xception(input_shape,n_classes):
    base_model = tf.keras.applications.Xception(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_VGG16(input_shape,n_classes):
    base_model = tf.keras.applications.VGG16(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_VGG19(input_shape,n_classes):
    base_model = tf.keras.applications.VGG19(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_ResNet50(input_shape,n_classes):
    base_model = tf.keras.applications.ResNet50(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_ResNet101(input_shape,n_classes):
    base_model = tf.keras.applications.ResNet101(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_ResNet152(input_shape,n_classes):
    base_model = tf.keras.applications.ResNet152(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_ResNet50V2(input_shape,n_classes):
    base_model = tf.keras.applications.ResNet50V2(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_ResNet101V2(input_shape,n_classes):
    base_model = tf.keras.applications.ResNet101V2(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_ResNet152V2(input_shape,n_classes):
    base_model = tf.keras.applications.ResNet152V2(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_InceptionResNetV2(input_shape,n_classes):
    base_model = tf.keras.applications.InceptionResNetV2(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_InceptionV3(input_shape,n_classes):
    base_model = tf.keras.applications.InceptionV3(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_MobileNet(input_shape,n_classes):
    base_model = tf.keras.applications.MobileNet(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = True

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_DenseNet121(input_shape,n_classes):
    base_model = tf.keras.applications.DenseNet121(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_DenseNet169(input_shape,n_classes):
    base_model = tf.keras.applications.densenet.DenseNet169(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = True

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.4)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_DenseNet201(input_shape,n_classes):
    base_model = tf.keras.applications.DenseNet201(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_EfficientNetB0(input_shape,n_classes):
    base_model = tf.keras.applications.EfficientNetB0(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_EfficientNetB1(input_shape,n_classes):
    base_model = tf.keras.applications.EfficientNetB1(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_EfficientNetB2(input_shape,n_classes):
    base_model = tf.keras.applications.EfficientNetB2(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_EfficientNetB3(input_shape,n_classes):
    base_model = tf.keras.applications.EfficientNetB3(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_EfficientNetB4(input_shape,n_classes):
    base_model = tf.keras.applications.EfficientNetB4(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_EfficientNetB5(input_shape,n_classes):
    base_model = tf.keras.applications.EfficientNetB5(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_EfficientNetB6(input_shape,n_classes):
    base_model = tf.keras.applications.EfficientNetB6(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_EfficientNetB7(input_shape,n_classes):
    base_model = tf.keras.applications.EfficientNetB7(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_NASNetMobile(input_shape,n_classes):
    base_model = tf.keras.applications.NASNetMobile(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def pretrained_NASNetLarge(input_shape,n_classes):
    base_model = tf.keras.applications.NASNetLarge(
        weights = "imagenet",
        input_shape = input_shape,
        include_top = False
    )

    base_model.trainable = False

    inputs = keras.Input(shape = input_shape)
    x = base_model(inputs,training = False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
    outputs = Dense(n_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs) 

    return model

def give_all_model_name():
    
    model_list = []
    
    model_list.append("proposed_model_1")
    model_list.append("proposed_model_2")
    model_list.append("soyNet")
    model_list.append("xception_network")
    model_list.append("densenet_121_network")
    model_list.append("densenet_169_network")
    model_list.append("densenet_201_network")
    model_list.append("vgg_16")
    model_list.append("vgg_19")
    model_list.append("resnet_50")
    model_list.append("resnet_101")
    model_list.append("resnet_152")
    model_list.append("squeezenet")
    model_list.append("alexnet")
    model_list.append("pretrained_Xception")
    model_list.append("pretrained_VGG16")
    model_list.append("pretrained_VGG19")
    model_list.append("pretrained_ResNet50")
    model_list.append("pretrained_ResNet101")
    model_list.append("pretrained_ResNet152")
    model_list.append("pretrained_ResNet50V2")
    model_list.append("pretrained_ResNet101V2")
    model_list.append("pretrained_ResNet152V2")
    model_list.append("pretrained_InceptionResNetV2")
    model_list.append("pretrained_InceptionV3")
    model_list.append("pretrained_MobileNet")
    model_list.append("pretrained_DenseNet121")
    model_list.append("pretrained_DenseNet169")
    model_list.append("pretrained_DenseNet201")
    model_list.append("pretrained_EfficientNetB0")
    model_list.append("pretrained_EfficientNetB1")
    model_list.append("pretrained_EfficientNetB2")
    model_list.append("pretrained_EfficientNetB3")
    model_list.append("pretrained_EfficientNetB4")
    model_list.append("pretrained_EfficientNetB5")
    model_list.append("pretrained_EfficientNetB6")
    model_list.append("pretrained_EfficientNetB7")
    model_list.append("pretrained_NASNetMobile")
    model_list.append("pretrained_NASNetLarge")
    
    print("Model name in this file: ")
    for model in model_list:
        print(model)

