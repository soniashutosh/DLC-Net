{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca9f3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:24:30.956655: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:30.956697: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'cucumber_data_report/import file')\n",
    "\n",
    "import model_import_file\n",
    "import evaluation_metrics_import_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0468a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"dataset\"\n",
    "path_to_save = \"cucumber_data_report/data_save_with_aug/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f1abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 23})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec72ee5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:24:34.387207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:34.387321: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:34.387413: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:34.387487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:34.387558: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:34.387625: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:34.387691: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:34.387760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:24:34.387773: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-01 13:24:34.388231: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train : 2814\n",
      "Size of y_train : 2814\n",
      "Size of X_test : 222\n",
      "Size of y_test : 222\n",
      "Proposed Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 686ms/step - loss: 0.1779 - accuracy: 0.9234\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 92.3423   | 92.3423   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  7.65766  |  7.65766  | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.981818 |  0.866071 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.878049 |  0.979798 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.878049 |  0.979798 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.979798 |  0.878049 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.927039 |  0.919431 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.980808 |  0.87204  | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.856364 |  0.854242 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "Soynet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 86ms/step - loss: 0.3743 - accuracy: 0.8468\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 84.6847   | 84.6847   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 15.3153   | 15.3153   | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.880342 |  0.809524 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.837398 |  0.858586 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.837398 |  0.858586 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.858586 |  0.837398 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.858333 |  0.833333 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.869396 |  0.823343 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.716302 |  0.715321 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "Xception Network Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 6s 717ms/step - loss: 0.4331 - accuracy: 0.8333\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 83.3333   | 83.3333   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 16.6667   | 16.6667   | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.846774 |  0.816327 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.853659 |  0.808081 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.853659 |  0.808081 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.808081 |  0.853659 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.850202 |  0.812183 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.827201 |  0.834784 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.690452 |  0.690617 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "DenseNet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 789ms/step - loss: 0.1646 - accuracy: 0.9279\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 92.7928   | 92.7928   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  7.20721  |  7.20721  | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.908397 |  0.956044 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.96748  |  0.878788 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.96748  |  0.878788 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.878788 |  0.96748  | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.937008 |  0.915789 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.89347  |  0.961745 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.858626 |  0.859954 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "Alexnet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 128ms/step - loss: 0.6445 - accuracy: 0.8333\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 83.3333   | 83.3333   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 16.6667   | 16.6667   | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.852459 |  0.81     | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.845528 |  0.818182 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.845528 |  0.818182 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.818182 |  0.845528 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.84898  |  0.81407  | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.835145 |  0.827574 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.691226 |  0.691062 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "VGG-16 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 11s 1s/step - loss: 0.4951 - accuracy: 0.8378\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 83.7838   | 83.7838   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 16.2162   | 16.2162   | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.967742 |  0.744186 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.731707 |  0.969697 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.731707 |  0.969697 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.969697 |  0.731707 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.833333 |  0.842105 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.968719 |  0.73792  | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.717319 |  0.712355 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "Resnet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 7s 730ms/step - loss: 0.6903 - accuracy: 0.5495\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 54.955    | 54.955    | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 45.045    | 45.045    | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.755556 |  0.497175 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.276423 |  0.888889 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.276423 |  0.888889 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.888889 |  0.276423 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.404762 |  0.637681 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.819515 |  0.370716 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.311728 |  0.295887 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "Efficient-Net B2 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 6s 484ms/step - loss: 0.6973 - accuracy: 0.4459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cucumber_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "cucumber_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-------------+----------------------------------+\n",
      "|   Class 1 |     Class 2 | Measure                          |\n",
      "+===========+=============+==================================+\n",
      "|   44.5946 |   44.5946   | Accuracy                         |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|   55.4054 |   55.4054   | Error                            |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|  nan      |    0.445946 | Precision                        |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|    0      |    1        | Recall                           |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|    0      |    1        | Sensitivity                      |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|    1      |    0        | Specificity                      |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|  nan      |    0.616822 | F1 Score                         |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|  nan      |    0        | G Mean                           |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|  inf      | -inf        | Matthews Correlation Coefficient |\n",
      "+-----------+-------------+----------------------------------+\n",
      "save_histogram_for_training_and_testing_accuracy(model_list,training_acc,testing_acc,file_path)\n",
      "save_of_training_accuracy_vs_epochs(model_history,epochs,file_path)\n",
      "save_of_training_loss_vs_epochs(model_history,epochs,file_path)\n",
      "print_evaluation_metrics(model,X_test_numpy_array,y_test,file_path)\n",
      "save_conf_matrix(model,X_test,y_test,class_list,file_path)\n",
      "save_histogram_for_training_and_testing_data(class_list,training_data_dict,testing_data_dict,file_path)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "import json,codecs\n",
    "\n",
    "def saveHist(path,history):\n",
    "    \n",
    "    new_hist = {}\n",
    "    for key in list(history.history.keys()):\n",
    "        new_hist[key]=history.history[key]\n",
    "        if type(history.history[key]) == np.ndarray:\n",
    "            new_hist[key] = history.history[key].tolist()\n",
    "        elif type(history.history[key]) == list:\n",
    "            if  type(history.history[key][0]) == np.float64:\n",
    "                new_hist[key] = list(map(float, history.history[key]))\n",
    "            \n",
    "    print(new_hist)\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(new_hist, file, separators=(',', ':'), sort_keys=True, indent=4) \n",
    "\n",
    "def loadHist(path):\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as file:\n",
    "        n = json.loads(file.read())\n",
    "    return n\n",
    "\n",
    "class_dict = {}\n",
    "\n",
    "counter = 0\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        class_dict[subfolder] = counter\n",
    "        counter += 1\n",
    "    break\n",
    "\n",
    "print(counter)\n",
    "class_dict\n",
    "\n",
    "class_list = []\n",
    "for ele in class_dict:\n",
    "    class_list.append(ele)\n",
    "\n",
    "class_list\n",
    "\n",
    "import random\n",
    "\n",
    "def image_augmentation(img):\n",
    "    central_cropped_image = tf.image.central_crop(img_array, central_fraction=0.9)\n",
    "    random_left_right_flip = tf.image.stateless_random_flip_left_right(img,(2,3))\n",
    "    random_up_down_flip = tf.image.stateless_random_flip_up_down(img,(2,3))\n",
    "    random_contrast = tf.image.stateless_random_contrast(img,0.5, 0.9,(2,3))\n",
    "    random_saturation = tf.image.stateless_random_saturation(img,0.5, 0.9,(2,3))\n",
    "    random_brightness = tf.image.stateless_random_brightness(img,0.2,(2,3))\n",
    "        \n",
    "    imgs = [central_cropped_image, random_left_right_flip, random_up_down_flip, random_contrast, random_saturation, random_brightness]\n",
    "    return imgs\n",
    "\n",
    "\n",
    "IMG_SIZE_X = 200\n",
    "IMG_SIZE_Y = 200\n",
    "\n",
    "total_training_images_classwise = {}\n",
    "total_testing_images_classwise ={}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        path_to_subfolder = os.path.join(path,subfolder)\n",
    "        key = subfolder\n",
    "        number_of_image_in_folder = len(os.listdir(path_to_subfolder))\n",
    "        if folder == \"testing\":\n",
    "            total_testing_images_classwise[subfolder] = number_of_image_in_folder\n",
    "        else:\n",
    "            total_training_images_classwise[subfolder] = 6*number_of_image_in_folder\n",
    "        for img in os.listdir(path_to_subfolder):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path_to_subfolder,img))\n",
    "                \n",
    "                if folder == \"testing\":\n",
    "                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                    X_test.append(resized_array)\n",
    "                    y_test.append(class_dict[subfolder])\n",
    "                else:\n",
    "                    augmented_image = image_augmentation(img_array)\n",
    "                    for imgs in augmented_image:\n",
    "                      numpy_image = imgs.numpy().astype(\"uint8\")\n",
    "                      resized_array = cv2.resize(numpy_image, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                      X_train.append(resized_array)\n",
    "                      y_train.append(class_dict[subfolder])\n",
    "            except Exception as e:\n",
    "                print(\"Exception is : \"+e)\n",
    "                pass\n",
    "            \n",
    "print(\"Size of X_train : \" + str(len(X_train)))\n",
    "print(\"Size of y_train : \" + str(len(y_train)))\n",
    "print(\"Size of X_test : \" + str(len(X_test)))\n",
    "print(\"Size of y_test : \" + str(len(y_test)))\n",
    "\n",
    "file_path = path_to_save\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_data(class_list,total_training_images_classwise,total_testing_images_classwise,file_path+\"histogram_for_training_and_testing_data.jpg\")\n",
    "\n",
    "X_train_numpy_array = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test_numpy_array = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_numpy_array = X_train_numpy_array/255\n",
    "X_test_numpy_array = X_test_numpy_array/255\n",
    "\n",
    "input_shape = IMG_SIZE_X, IMG_SIZE_Y, 3\n",
    "n_classes = counter\n",
    "epoch = 50\n",
    "# Adagrad\n",
    "adagrad_opt = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.0005,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    name=\"Adagrad\",\n",
    ")\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "model_history = []\n",
    "\n",
    "model_name = \"proposed_model_1\"\n",
    "print(\"Proposed Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    proposed_model_1 = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    proposed_model_1.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    proposed_model_1_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    proposed_model_1 = model_import_file.proposed_model_1(input_shape,n_classes)\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    proposed_model_1_history = proposed_model_1.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = proposed_model_1.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    proposed_model_1.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",proposed_model_1_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "proposed_model_1_eval = proposed_model_1.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(proposed_model_1_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"proposed_model_1\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(proposed_model_1,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(proposed_model_1,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "# For deleting files if not wanted\n",
    "# model_name = \"proposed_model_1\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "    os.remove(path_to_save + model_name + '.json')\n",
    "    os.remove(path_to_save + model_name + '.h5')\n",
    "    os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_name = \"soyNet\"\n",
    "print(\"Soynet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    soyNet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    soyNet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    soyNet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    soyNet_model = model_import_file.soyNet(input_shape,n_classes)\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    soyNet_model_history = soyNet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = soyNet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    soyNet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",soyNet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "soyNet_model_eval = soyNet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "# For deleting files if not wanted\n",
    "model_name = \"soyNet\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "  os.remove(path_to_save + model_name + '.json')\n",
    "  os.remove(path_to_save + model_name + '.h5')\n",
    "  os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_history.append(soyNet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"soyNet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(soyNet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(soyNet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"xception_network\"\n",
    "print(\"Xception Network Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    xception_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    xception_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    xception_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    xception_model = model_import_file.xception_network(input_shape,n_classes)\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    xception_model_history = xception_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = xception_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    xception_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",xception_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "xception_model_eval = xception_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(xception_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"xception_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(xception_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(xception_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"densenet_121_network\"\n",
    "print(\"DenseNet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    densenet_121_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    densenet_121_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    densenet_121_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    densenet_121_model = model_import_file.densenet_121_network(input_shape,n_classes)\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    densenet_121_model_history = densenet_121_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = densenet_121_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    densenet_121_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",densenet_121_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "densenet_121_model_eval = densenet_121_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(densenet_121_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"densenet_121_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(densenet_121_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(densenet_121_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"alexnet\"\n",
    "print(\"Alexnet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    alexnet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    alexnet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    alexnet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    alexnet_model = model_import_file.alexnet(input_shape,n_classes)\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    alexnet_model_history = alexnet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = alexnet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    alexnet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",alexnet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "alexnet_model_eval = alexnet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(alexnet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"alexnet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(alexnet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(alexnet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_VGG16\"\n",
    "print(\"VGG-16 Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_VGG16_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_VGG16_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_VGG16_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_VGG16_model = model_import_file.pretrained_VGG16(input_shape,n_classes)\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_VGG16_model_history = pretrained_VGG16_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_VGG16_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_VGG16_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_VGG16_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_VGG16_model_eval = pretrained_VGG16_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_VGG16_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_VGG16\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_VGG16_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_VGG16_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "print(\"Resnet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_ResNet50_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_ResNet50_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_ResNet50_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_ResNet50_model = model_import_file.pretrained_ResNet50(input_shape,n_classes)\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_ResNet50_model_history = pretrained_ResNet50_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_ResNet50_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_ResNet50_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_ResNet50_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_ResNet50_model_eval = pretrained_ResNet50_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_ResNet50_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_ResNet50_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_ResNet50_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "print(\"Efficient-Net B2 Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_EfficientNetB2_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_EfficientNetB2_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_EfficientNetB2_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_EfficientNetB2_model = model_import_file.pretrained_EfficientNetB2(input_shape,n_classes)\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_EfficientNetB2_model_history = pretrained_EfficientNetB2_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_EfficientNetB2_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_EfficientNetB2_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_EfficientNetB2_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_EfficientNetB2_model_eval = pretrained_EfficientNetB2_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_EfficientNetB2_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "evaluation_metrics_import_file.save_of_training_accuracy_vs_epochs(model_history,epoch,file_path+\"training_accuracy_vs_epochs\")\n",
    "evaluation_metrics_import_file.save_of_training_loss_vs_epochs(model_history,epoch,file_path+\"training_loss_vs_epochs\")\n",
    "\n",
    "evaluation_metrics_import_file.func_help()\n",
    "\n",
    "training_accuracy_of_different_models = []\n",
    "testing_accuracy_of_different_models = []\n",
    "model_list = [\"Proposed Model 1\",\"SoyNet Model\",\"Xception Model\",\"Densenet 121 Model\",\"AlexNet Model\",\"VGG16 Model\",\"ResNet 50 Model\",\"EfficientNetB2 Model\"]\n",
    "\n",
    "sz = len(model_history)\n",
    "for i in range(sz):\n",
    "    length = len(model_history[i]['accuracy'])\n",
    "    training_accuracy_of_different_models.append(model_history[i]['accuracy'][length-1])\n",
    "\n",
    "\n",
    "testing_accuracy_of_different_models.append(proposed_model_1_eval[1])\n",
    "testing_accuracy_of_different_models.append(soyNet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(xception_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(densenet_121_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(alexnet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_VGG16_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_ResNet50_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_EfficientNetB2_model_eval[1])\n",
    "\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_accuracy(model_list,training_accuracy_of_different_models,testing_accuracy_of_different_models,file_path+\"training_and_testing_accuracy.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f9101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d085e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
