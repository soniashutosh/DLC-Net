{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91827d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:16:01.560154: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:01.560186: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'cucumber_data_report/import file')\n",
    "\n",
    "import model_import_file\n",
    "import evaluation_metrics_import_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33f933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"dataset\"\n",
    "path_to_save = \"cucumber_data_report/data_save_without_aug/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ded2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 23})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab5266b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ill_cucumber', 'good_Cucumber']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "import json,codecs\n",
    "\n",
    "def saveHist(path,history):\n",
    "    \n",
    "    new_hist = {}\n",
    "    for key in list(history.history.keys()):\n",
    "        new_hist[key]=history.history[key]\n",
    "        if type(history.history[key]) == np.ndarray:\n",
    "            new_hist[key] = history.history[key].tolist()\n",
    "        elif type(history.history[key]) == list:\n",
    "            if  type(history.history[key][0]) == np.float64:\n",
    "                new_hist[key] = list(map(float, history.history[key]))\n",
    "            \n",
    "    print(new_hist)\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(new_hist, file, separators=(',', ':'), sort_keys=True, indent=4) \n",
    "\n",
    "def loadHist(path):\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as file:\n",
    "        n = json.loads(file.read())\n",
    "    return n\n",
    "\n",
    "class_dict = {}\n",
    "\n",
    "counter = 0\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        class_dict[subfolder] = counter\n",
    "        counter += 1\n",
    "    break\n",
    "\n",
    "print(counter)\n",
    "class_dict\n",
    "\n",
    "class_list = []\n",
    "for ele in class_dict:\n",
    "    class_list.append(ele)\n",
    "\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607450f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train : 469\n",
      "Size of y_train : 469\n",
      "Size of X_test : 222\n",
      "Size of y_test : 222\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE_X = 200\n",
    "IMG_SIZE_Y = 200\n",
    "\n",
    "total_training_images_classwise = {}\n",
    "total_testing_images_classwise ={}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        path_to_subfolder = os.path.join(path,subfolder)\n",
    "        key = subfolder\n",
    "        number_of_image_in_folder = len(os.listdir(path_to_subfolder))\n",
    "        if folder == \"testing\":\n",
    "            total_testing_images_classwise[subfolder] = number_of_image_in_folder\n",
    "        else:\n",
    "            total_training_images_classwise[subfolder] = number_of_image_in_folder\n",
    "        for img in os.listdir(path_to_subfolder):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path_to_subfolder,img))\n",
    "                if folder == \"testing\":\n",
    "                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                    X_test.append(resized_array)\n",
    "                    y_test.append(class_dict[subfolder])\n",
    "                else:\n",
    "                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                    X_train.append(resized_array)\n",
    "                    y_train.append(class_dict[subfolder])\n",
    "            except Exception as e:\n",
    "                print(\"Exception is : \"+e)\n",
    "                pass\n",
    "            \n",
    "print(\"Size of X_train : \" + str(len(X_train)))\n",
    "print(\"Size of y_train : \" + str(len(y_train)))\n",
    "print(\"Size of X_test : \" + str(len(X_test)))\n",
    "print(\"Size of y_test : \" + str(len(y_test)))\n",
    "\n",
    "file_path = path_to_save\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_data(class_list,total_training_images_classwise,total_testing_images_classwise,file_path+\"histogram_for_training_and_testing_data.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddb82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:16:56.611308: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:56.611454: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:56.611544: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:56.611619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:56.611694: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:56.611766: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:56.611837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:56.611908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-01 13:16:56.611920: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-01 13:16:56.612270: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 711ms/step - loss: 0.2828 - accuracy: 0.9054\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 90.5405   | 90.5405   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  9.45946  |  9.45946  | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.91129  |  0.897959 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.918699 |  0.888889 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.918699 |  0.888889 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.888889 |  0.918699 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.91498  |  0.893401 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.90002  |  0.90827  | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.817379 |  0.817543 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "SoyNet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 84ms/step - loss: 0.3566 - accuracy: 0.8604\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 86.036    | 86.036    | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 13.964    | 13.964    | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.833333 |  0.904762 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.934959 |  0.767677 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.934959 |  0.767677 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.767677 |  0.934959 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.881226 |  0.830601 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.799832 |  0.919737 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.734372 |  0.736897 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "Xception Network\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 6s 690ms/step - loss: 0.3835 - accuracy: 0.8604\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 86.036    | 86.036    | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 13.964    | 13.964    | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.896552 |  0.820755 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.845528 |  0.878788 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.845528 |  0.878788 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.878788 |  0.845528 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.870293 |  0.84878  | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.887625 |  0.833049 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.740008 |  0.738864 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "DenseNet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 785ms/step - loss: 0.3266 - accuracy: 0.8649\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 86.4865   | 86.4865   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 13.5135   | 13.5135   | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.884298 |  0.841584 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.869919 |  0.858586 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.869919 |  0.858586 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.858586 |  0.869919 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.877049 |  0.85     | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.871347 |  0.855634 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.745718 |  0.74539  | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "AlexNet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 137ms/step - loss: 0.7529 - accuracy: 0.7928\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 79.2793   | 79.2793   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 20.7207   | 20.7207   | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.846847 |  0.738739 | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.764228 |  0.828283 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.764228 |  0.828283 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.828283 |  0.764228 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.803419 |  0.780952 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.837513 |  0.751375 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.630266 |  0.628307 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "VGG-16 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 11s 1s/step - loss: 0.5855 - accuracy: 0.8559\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+----------------------------------+\n",
      "|   Class 1 |   Class 2 | Measure                          |\n",
      "+===========+===========+==================================+\n",
      "| 85.5856   | 85.5856   | Accuracy                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "| 14.4144   | 14.4144   | Error                            |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.917431 |  0.79646  | Precision                        |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.813008 |  0.909091 | Recall                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.813008 |  0.909091 | Sensitivity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.909091 |  0.813008 | Specificity                      |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.862069 |  0.849057 | F1 Score                         |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.913252 |  0.804692 | G Mean                           |\n",
      "+-----------+-----------+----------------------------------+\n",
      "|  0.736029 |  0.733743 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+----------------------------------+\n",
      "ResNet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 7s 717ms/step - loss: 0.6937 - accuracy: 0.4459\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+------------+------------+----------------------------------+\n",
      "|    Class 1 |    Class 2 | Measure                          |\n",
      "+============+============+==================================+\n",
      "| 44.5946    | 44.5946    | Accuracy                         |\n",
      "+------------+------------+----------------------------------+\n",
      "| 55.4054    | 55.4054    | Error                            |\n",
      "+------------+------------+----------------------------------+\n",
      "|  0.5       |  0.444954  | Precision                        |\n",
      "+------------+------------+----------------------------------+\n",
      "|  0.0162602 |  0.979798  | Recall                           |\n",
      "+------------+------------+----------------------------------+\n",
      "|  0.0162602 |  0.979798  | Sensitivity                      |\n",
      "+------------+------------+----------------------------------+\n",
      "|  0.979798  |  0.0162602 | Specificity                      |\n",
      "+------------+------------+----------------------------------+\n",
      "|  0.0314961 |  0.611987  | F1 Score                         |\n",
      "+------------+------------+----------------------------------+\n",
      "|  0.699928  |  0.085059  | G Mean                           |\n",
      "+------------+------------+----------------------------------+\n",
      "|  0.0960541 |  0.0230162 | Matthews Correlation Coefficient |\n",
      "+------------+------------+----------------------------------+\n",
      "EfficientNet B2 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 7s 498ms/step - loss: 0.6969 - accuracy: 0.4459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cucumber_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "cucumber_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-------------+----------------------------------+\n",
      "|   Class 1 |     Class 2 | Measure                          |\n",
      "+===========+=============+==================================+\n",
      "|   44.5946 |   44.5946   | Accuracy                         |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|   55.4054 |   55.4054   | Error                            |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|  nan      |    0.445946 | Precision                        |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|    0      |    1        | Recall                           |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|    0      |    1        | Sensitivity                      |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|    1      |    0        | Specificity                      |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|  nan      |    0.616822 | F1 Score                         |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|  nan      |    0        | G Mean                           |\n",
      "+-----------+-------------+----------------------------------+\n",
      "|  inf      | -inf        | Matthews Correlation Coefficient |\n",
      "+-----------+-------------+----------------------------------+\n",
      "save_histogram_for_training_and_testing_accuracy(model_list,training_acc,testing_acc,file_path)\n",
      "save_of_training_accuracy_vs_epochs(model_history,epochs,file_path)\n",
      "save_of_training_loss_vs_epochs(model_history,epochs,file_path)\n",
      "print_evaluation_metrics(model,X_test_numpy_array,y_test,file_path)\n",
      "save_conf_matrix(model,X_test,y_test,class_list,file_path)\n",
      "save_histogram_for_training_and_testing_data(class_list,training_data_dict,testing_data_dict,file_path)\n"
     ]
    }
   ],
   "source": [
    "X_train_numpy_array = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test_numpy_array = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_numpy_array = X_train_numpy_array/255\n",
    "X_test_numpy_array = X_test_numpy_array/255\n",
    "\n",
    "input_shape = IMG_SIZE_X, IMG_SIZE_Y, 3\n",
    "n_classes = counter\n",
    "epoch = 50\n",
    "# Adagrad\n",
    "adagrad_opt = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.0005,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    name=\"Adagrad\",\n",
    ")\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "model_history = []\n",
    "\n",
    "model_name = \"proposed_model_1\"\n",
    "print(\"Proposed Model 1\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    proposed_model_1 = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    proposed_model_1.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    proposed_model_1_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    proposed_model_1 = model_import_file.proposed_model_1(input_shape,n_classes)\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    proposed_model_1_history = proposed_model_1.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = proposed_model_1.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    proposed_model_1.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",proposed_model_1_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "proposed_model_1_eval = proposed_model_1.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(proposed_model_1_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"proposed_model_1\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(proposed_model_1,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(proposed_model_1,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "# For deleting files if not wanted\n",
    "# model_name = \"proposed_model_1\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "    os.remove(path_to_save + model_name + '.json')\n",
    "    os.remove(path_to_save + model_name + '.h5')\n",
    "    os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_name = \"soyNet\"\n",
    "print(\"SoyNet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    soyNet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    soyNet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    soyNet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    soyNet_model = model_import_file.soyNet(input_shape,n_classes)\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    soyNet_model_history = soyNet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = soyNet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    soyNet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",soyNet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "soyNet_model_eval = soyNet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "# For deleting files if not wanted\n",
    "model_name = \"soyNet\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "  os.remove(path_to_save + model_name + '.json')\n",
    "  os.remove(path_to_save + model_name + '.h5')\n",
    "  os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_history.append(soyNet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"soyNet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(soyNet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(soyNet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"xception_network\"\n",
    "print(\"Xception Network\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    xception_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    xception_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    xception_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    xception_model = model_import_file.xception_network(input_shape,n_classes)\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    xception_model_history = xception_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = xception_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    xception_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",xception_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "xception_model_eval = xception_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(xception_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"xception_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(xception_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(xception_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"densenet_121_network\"\n",
    "print(\"DenseNet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    densenet_121_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    densenet_121_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    densenet_121_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    densenet_121_model = model_import_file.densenet_121_network(input_shape,n_classes)\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    densenet_121_model_history = densenet_121_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = densenet_121_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    densenet_121_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",densenet_121_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "densenet_121_model_eval = densenet_121_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(densenet_121_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"densenet_121_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(densenet_121_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(densenet_121_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"alexnet\"\n",
    "print(\"AlexNet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    alexnet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    alexnet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    alexnet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    alexnet_model = model_import_file.alexnet(input_shape,n_classes)\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    alexnet_model_history = alexnet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = alexnet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    alexnet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",alexnet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "alexnet_model_eval = alexnet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(alexnet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"alexnet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(alexnet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(alexnet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_VGG16\"\n",
    "print(\"VGG-16 Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_VGG16_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_VGG16_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_VGG16_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_VGG16_model = model_import_file.pretrained_VGG16(input_shape,n_classes)\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_VGG16_model_history = pretrained_VGG16_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_VGG16_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_VGG16_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_VGG16_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_VGG16_model_eval = pretrained_VGG16_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_VGG16_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_VGG16\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_VGG16_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_VGG16_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "print(\"ResNet Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_ResNet50_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_ResNet50_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_ResNet50_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_ResNet50_model = model_import_file.pretrained_ResNet50(input_shape,n_classes)\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_ResNet50_model_history = pretrained_ResNet50_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_ResNet50_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_ResNet50_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_ResNet50_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_ResNet50_model_eval = pretrained_ResNet50_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_ResNet50_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_ResNet50_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_ResNet50_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "print(\"EfficientNet B2 Model\")\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_EfficientNetB2_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_EfficientNetB2_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_EfficientNetB2_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_EfficientNetB2_model = model_import_file.pretrained_EfficientNetB2(input_shape,n_classes)\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_EfficientNetB2_model_history = pretrained_EfficientNetB2_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_EfficientNetB2_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_EfficientNetB2_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_EfficientNetB2_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_EfficientNetB2_model_eval = pretrained_EfficientNetB2_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_EfficientNetB2_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "evaluation_metrics_import_file.save_of_training_accuracy_vs_epochs(model_history,epoch,file_path+\"training_accuracy_vs_epochs\")\n",
    "evaluation_metrics_import_file.save_of_training_loss_vs_epochs(model_history,epoch,file_path+\"training_loss_vs_epochs\")\n",
    "\n",
    "evaluation_metrics_import_file.func_help()\n",
    "\n",
    "training_accuracy_of_different_models = []\n",
    "testing_accuracy_of_different_models = []\n",
    "model_list = [\"Proposed Model 1\",\"SoyNet Model\",\"Xception Model\",\"Densenet 121 Model\",\"AlexNet Model\",\"VGG16 Model\",\"ResNet 50 Model\",\"EfficientNetB2 Model\"]\n",
    "\n",
    "sz = len(model_history)\n",
    "for i in range(sz):\n",
    "    length = len(model_history[i]['accuracy'])\n",
    "    training_accuracy_of_different_models.append(model_history[i]['accuracy'][length-1])\n",
    "\n",
    "\n",
    "testing_accuracy_of_different_models.append(proposed_model_1_eval[1])\n",
    "testing_accuracy_of_different_models.append(soyNet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(xception_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(densenet_121_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(alexnet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_VGG16_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_ResNet50_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_EfficientNetB2_model_eval[1])\n",
    "\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_accuracy(model_list,training_accuracy_of_different_models,testing_accuracy_of_different_models,file_path+\"training_and_testing_accuracy.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667d4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f7c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
