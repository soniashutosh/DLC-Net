{"cells":[{"cell_type":"code","execution_count":null,"id":"f8d1ae50","metadata":{"id":"f8d1ae50","outputId":"be5ac2df-b903-48fb-eded-80c0f69eae86"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-05-01 13:32:42.533986: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n","2022-05-01 13:32:42.534028: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]}],"source":["import sys\n","sys.path.insert(0, 'Grapes_data_report/import file')\n","\n","import model_import_file\n","import evaluation_metrics_import_file"]},{"cell_type":"code","execution_count":null,"id":"646a7783","metadata":{"id":"646a7783"},"outputs":[],"source":["path_to_data = \"dataset\"\n","path_to_save = \"Grapes_data_report/data_save_without_aug/\""]},{"cell_type":"code","execution_count":null,"id":"8b54fb06","metadata":{"id":"8b54fb06"},"outputs":[],"source":["import matplotlib\n","matplotlib.rcParams.update({'font.size': 23})"]},{"cell_type":"code","execution_count":null,"id":"85769b0c","metadata":{"id":"85769b0c","outputId":"a504ac45-46db-436c-af6d-c8e6838add1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n"]},{"data":{"text/plain":["['Grape___healthy',\n"," 'Grape___Esca_(Black_Measles)',\n"," 'Grape___Black_rot',\n"," 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import time\n","\n","import tensorflow as tf\n","from keras.models import model_from_json\n","import json,codecs\n","\n","def saveHist(path,history):\n","    \n","    new_hist = {}\n","    for key in list(history.history.keys()):\n","        new_hist[key]=history.history[key]\n","        if type(history.history[key]) == np.ndarray:\n","            new_hist[key] = history.history[key].tolist()\n","        elif type(history.history[key]) == list:\n","            if  type(history.history[key][0]) == np.float64:\n","                new_hist[key] = list(map(float, history.history[key]))\n","            \n","    print(new_hist)\n","    with codecs.open(path, 'w', encoding='utf-8') as file:\n","        json.dump(new_hist, file, separators=(',', ':'), sort_keys=True, indent=4) \n","\n","def loadHist(path):\n","    with codecs.open(path, 'r', encoding='utf-8') as file:\n","        n = json.loads(file.read())\n","    return n\n","\n","class_dict = {}\n","\n","counter = 0\n","for folder in os.listdir(path_to_data):\n","    path = os.path.join(path_to_data,folder)\n","    for subfolder in os.listdir(path):\n","        class_dict[subfolder] = counter\n","        counter += 1\n","    break\n","\n","print(counter)\n","class_dict\n","\n","class_list = []\n","for ele in class_dict:\n","    class_list.append(ele)\n","\n","class_list"]},{"cell_type":"code","execution_count":null,"id":"98df85e4","metadata":{"id":"98df85e4","outputId":"35f5e06e-b520-4957-d5c5-a9e515b5700a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of X_train : 7222\n","Size of y_train : 7222\n","Size of X_test : 1805\n","Size of y_test : 1805\n"]}],"source":["IMG_SIZE_X = 200\n","IMG_SIZE_Y = 200\n","\n","total_training_images_classwise = {}\n","total_testing_images_classwise ={}\n","\n","X_train = []\n","y_train = []\n","\n","X_test = []\n","y_test = []\n","\n","for folder in os.listdir(path_to_data):\n","    path = os.path.join(path_to_data,folder)\n","    for subfolder in os.listdir(path):\n","        path_to_subfolder = os.path.join(path,subfolder)\n","        key = subfolder\n","        number_of_image_in_folder = len(os.listdir(path_to_subfolder))\n","        if folder == \"validation\":\n","            total_testing_images_classwise[subfolder] = number_of_image_in_folder\n","        else:\n","            total_training_images_classwise[subfolder] = number_of_image_in_folder\n","        for img in os.listdir(path_to_subfolder):\n","            try:\n","                img_array = cv2.imread(os.path.join(path_to_subfolder,img))\n","                if folder == \"validation\":\n","                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n","                    X_test.append(resized_array)\n","                    y_test.append(class_dict[subfolder])\n","                else:\n","                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n","                    X_train.append(resized_array)\n","                    y_train.append(class_dict[subfolder])\n","            except Exception as e:\n","                print(\"Exception is : \"+e)\n","                pass\n","            \n","print(\"Size of X_train : \" + str(len(X_train)))\n","print(\"Size of y_train : \" + str(len(y_train)))\n","print(\"Size of X_test : \" + str(len(X_test)))\n","print(\"Size of y_test : \" + str(len(y_test)))\n","\n","file_path = path_to_save\n","evaluation_metrics_import_file.save_histogram_for_training_and_testing_data(class_list,total_training_images_classwise,total_testing_images_classwise,file_path+\"histogram_for_training_and_testing_data.jpg\")\n"]},{"cell_type":"code","execution_count":null,"id":"984982b5","metadata":{"id":"984982b5","outputId":"b3ebccff-5570-4a1a-a45b-6b870d276aaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Proposed Model\n"]},{"name":"stderr","output_type":"stream","text":["2022-05-01 13:33:03.411185: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n","2022-05-01 13:33:03.411470: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n","2022-05-01 13:33:03.411656: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n","2022-05-01 13:33:03.411831: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n","2022-05-01 13:33:03.411995: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n","2022-05-01 13:33:03.412153: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n","2022-05-01 13:33:03.412310: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n","2022-05-01 13:33:03.412478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n","2022-05-01 13:33:03.412509: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n","2022-05-01 13:33:03.413333: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"name":"stdout","output_type":"stream","text":["Loaded model from disk\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 49s 784ms/step - loss: 0.0153 - accuracy: 0.9950\n","\n","\n"," Report \n","\n","\n","\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|   Class 1 |   Class 2 |   Class 3 |    Class 4 | Measure                          |\n","+===========+===========+===========+============+==================================+\n","| 99.8892   | 99.5014   | 99.6676   | 99.9446    | Accuracy                         |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.110803 |  0.498615 |  0.33241  |  0.0554017 | Error                            |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.997636 |  0.991649 |  0.993644 |  0.99768   | Precision                        |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.997636 |  0.989583 |  0.993644 |  1         | Recall                           |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.997636 |  0.989583 |  0.993644 |  1         | Sensitivity                      |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.999276 |  0.996981 |  0.997749 |  0.999273  | Specificity                      |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.997636 |  0.990615 |  0.993644 |  0.998839  | F1 Score                         |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.998456 |  0.994312 |  0.995695 |  0.998476  | G Mean                           |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.996914 |  0.987254 |  0.991408 |  0.998474  | Matthews Correlation Coefficient |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","SoyNet Model\n","Loaded model from disk\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 6s 91ms/step - loss: 0.1603 - accuracy: 0.9330\n","\n","\n"," Report \n","\n","\n","\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n","+===========+===========+===========+===========+==================================+\n","| 99.2244   | 94.9584   | 93.518    | 98.892    | Accuracy                         |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.775623 |  5.04155  |  6.48199  |  1.10803  | Error                            |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.981176 |  0.947126 |  0.842004 |  0.981221 | Precision                        |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.985816 |  0.858333 |  0.925847 |  0.972093 | Recall                           |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.985816 |  0.858333 |  0.925847 |  0.972093 | Sensitivity                      |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.994211 |  0.982642 |  0.938485 |  0.994182 | Specificity                      |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.983491 |  0.900546 |  0.881937 |  0.976636 | F1 Score                         |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.987672 |  0.964721 |  0.888936 |  0.98768  | G Mean                           |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.978505 |  0.871387 |  0.843551 |  0.969561 | Matthews Correlation Coefficient |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","Xception Network Model\n","Loaded model from disk\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 45s 767ms/step - loss: 0.0239 - accuracy: 0.9934\n","\n","\n"," Report \n","\n","\n","\n","+------------+-----------+-----------+------------+----------------------------------+\n","|    Class 1 |   Class 2 |   Class 3 |    Class 4 | Measure                          |\n","+============+===========+===========+============+==================================+\n","| 99.9446    | 99.446    | 99.3352   | 99.9446    | Accuracy                         |\n","+------------+-----------+-----------+------------+----------------------------------+\n","|  0.0554017 |  0.554017 |  0.66482  |  0.0554017 | Error                            |\n","+------------+-----------+-----------+------------+----------------------------------+\n","|  1         |  0.985537 |  0.989362 |  1         | Precision                        |\n","+------------+-----------+-----------+------------+----------------------------------+\n","|  0.997636  |  0.99375  |  0.985169 |  0.997674  | Recall                           |\n","+------------+-----------+-----------+------------+----------------------------------+\n","|  0.997636  |  0.99375  |  0.985169 |  0.997674  | Sensitivity                      |\n","+------------+-----------+-----------+------------+----------------------------------+\n","|  1         |  0.994717 |  0.996249 |  1         | Specificity                      |\n","+------------+-----------+-----------+------------+----------------------------------+\n","|  0.998817  |  0.989627 |  0.987261 |  0.998836  | F1 Score                         |\n","+------------+-----------+-----------+------------+----------------------------------+\n","|  1         |  0.990116 |  0.992799 |  1         | G Mean                           |\n","+------------+-----------+-----------+------------+----------------------------------+\n","|  0.998458  |  0.98589  |  0.982826 |  0.998475  | Matthews Correlation Coefficient |\n","+------------+-----------+-----------+------------+----------------------------------+\n","Densenet Model\n","Loaded model from disk\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 51s 826ms/step - loss: 0.0241 - accuracy: 0.9945\n","\n","\n"," Report \n","\n","\n","\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n","+===========+===========+===========+===========+==================================+\n","| 99.8892   | 99.5014   | 99.5014   |       100 | Accuracy                         |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.110803 |  0.498615 |  0.498615 |         0 | Error                            |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  1        |  0.991649 |  0.987368 |         1 | Precision                        |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.995272 |  0.989583 |  0.993644 |         1 | Recall                           |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.995272 |  0.989583 |  0.993644 |         1 | Sensitivity                      |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  1        |  0.996981 |  0.995499 |         1 | Specificity                      |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.99763  |  0.990615 |  0.990496 |         1 | F1 Score                         |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  1        |  0.994312 |  0.991425 |         1 | G Mean                           |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.996915 |  0.987254 |  0.98715  |         1 | Matthews Correlation Coefficient |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","Alexnet Model\n","Loaded model from disk\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 9s 143ms/step - loss: 0.0312 - accuracy: 0.9884\n","\n","\n"," Report \n","\n","\n","\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n","+===========+===========+===========+===========+==================================+\n","| 99.8892   | 99.2244   | 98.892    | 99.6676   | Accuracy                         |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.110803 |  0.775623 |  1.10803  |  0.33241  | Error                            |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.997636 |  0.989496 |  0.972803 |  0.995327 | Precision                        |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.997636 |  0.98125  |  0.985169 |  0.990698 | Recall                           |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.997636 |  0.98125  |  0.985169 |  0.990698 | Sensitivity                      |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.999276 |  0.996226 |  0.990248 |  0.998545 | Specificity                      |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.997636 |  0.985356 |  0.978947 |  0.993007 | F1 Score                         |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.998456 |  0.992855 |  0.981487 |  0.996935 | G Mean                           |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.996914 |  0.980174 |  0.971599 |  0.990848 | Matthews Correlation Coefficient |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","VGG-16 Model\n","Loaded model from disk\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 86s 1s/step - loss: 0.8338 - accuracy: 0.8975\n","\n","\n"," Report \n","\n","\n","\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|   Class 1 |   Class 2 |   Class 3 |   Class 4 | Measure                          |\n","+===========+===========+===========+===========+==================================+\n","| 97.0637   | 93.0194   | 90.5817   | 98.8366   | Accuracy                         |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  2.93629  |  6.98061  |  9.41828  |  1.16343  | Error                            |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.974359 |  0.835227 |  0.841629 |  0.959551 | Precision                        |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.898345 |  0.91875  |  0.788136 |  0.993023 | Recall                           |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.898345 |  0.91875  |  0.788136 |  0.993023 | Sensitivity                      |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.992764 |  0.93434  |  0.947487 |  0.986909 | Specificity                      |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.934809 |  0.875    |  0.814004 |  0.976    | F1 Score                         |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.983518 |  0.883395 |  0.892991 |  0.973134 | G Mean                           |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","|  0.917974 |  0.833643 |  0.763183 |  0.968638 | Matthews Correlation Coefficient |\n","+-----------+-----------+-----------+-----------+----------------------------------+\n","Resnet Model\n","Loaded model from disk\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 53s 850ms/step - loss: 1.2931 - accuracy: 0.4565\n","\n","\n"," Report \n","\n","\n","\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|   Class 1 |   Class 2 |   Class 3 |    Class 4 | Measure                          |\n","+===========+===========+===========+============+==================================+\n","| 82.4377   | 74.903    | 57.2299   | 76.7313    | Accuracy                         |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","| 17.5623   | 25.097    | 42.7701   | 23.2687    | Error                            |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.964912 |  0.516206 |  0.322695 |  0.916667  | Precision                        |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.260047 |  0.895833 |  0.57839  |  0.0255814 | Recall                           |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.260047 |  0.895833 |  0.57839  |  0.0255814 | Sensitivity                      |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.997106 |  0.695849 |  0.570143 |  0.999273  | Specificity                      |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.409683 |  0.654989 |  0.414264 |  0.0497738 | F1 Score                         |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.980877 |  0.599334 |  0.428931 |  0.957079  | G Mean                           |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","|  0.452458 |  0.551986 |  0.289876 |  0.137709  | Matthews Correlation Coefficient |\n","+-----------+-----------+-----------+------------+----------------------------------+\n","EfficeintNet-B2 Model\n","Loaded model from disk\n"]},{"name":"stderr","output_type":"stream","text":["/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 43s 665ms/step - loss: 1.3841 - accuracy: 0.2659\n"]},{"name":"stderr","output_type":"stream","text":["Grapes_data_report/import file/evaluation_metrics_import_file.py:82: RuntimeWarning: invalid value encountered in true_divide\n","  prec = (TP) / (TP + FP)\n","Grapes_data_report/import file/evaluation_metrics_import_file.py:111: RuntimeWarning: divide by zero encountered in true_divide\n","  return num/(np.sqrt(den))\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," Report \n","\n","\n","\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|   Class 1 |     Class 2 |   Class 3 |   Class 4 | Measure                          |\n","+===========+=============+===========+===========+==================================+\n","|   76.5651 |   26.5928   |   73.8504 |   76.1773 | Accuracy                         |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|   23.4349 |   73.4072   |   26.1496 |   23.8227 | Error                            |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|  nan      |    0.265928 |  nan      |  nan      | Precision                        |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|    0      |    1        |    0      |    0      | Recall                           |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|    0      |    1        |    0      |    0      | Sensitivity                      |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|    1      |    0        |    1      |    1      | Specificity                      |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|  nan      |    0.420131 |  nan      |  nan      | F1 Score                         |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|  nan      |    0        |  nan      |  nan      | G Mean                           |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","|  inf      | -inf        |  inf      |  inf      | Matthews Correlation Coefficient |\n","+-----------+-------------+-----------+-----------+----------------------------------+\n","save_histogram_for_training_and_testing_accuracy(model_list,training_acc,testing_acc,file_path)\n","save_of_training_accuracy_vs_epochs(model_history,epochs,file_path)\n","save_of_training_loss_vs_epochs(model_history,epochs,file_path)\n","print_evaluation_metrics(model,X_test_numpy_array,y_test,file_path)\n","save_conf_matrix(model,X_test,y_test,class_list,file_path)\n","save_histogram_for_training_and_testing_data(class_list,training_data_dict,testing_data_dict,file_path)\n"]}],"source":["X_train_numpy_array = np.array(X_train)\n","y_train = np.array(y_train)\n","X_test_numpy_array = np.array(X_test)\n","y_test = np.array(y_test)\n","\n","X_train_numpy_array = X_train_numpy_array/255\n","X_test_numpy_array = X_test_numpy_array/255\n","\n","input_shape = IMG_SIZE_X, IMG_SIZE_Y, 3\n","n_classes = counter\n","epoch = 50\n","# Adagrad\n","adagrad_opt = tf.keras.optimizers.Adagrad(\n","    learning_rate=0.0005,\n","    initial_accumulator_value=0.1,\n","    epsilon=1e-07,\n","    name=\"Adagrad\",\n",")\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n","\n","model_history = []\n","\n","model_name = \"proposed_model_1\"\n","print(\"Proposed Model\")\n","if os.path.exists(path_to_save + model_name + '.json'):\n","    json_file = open(path_to_save + model_name + '.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    proposed_model_1 = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    proposed_model_1.load_weights(path_to_save + model_name + \".h5\")\n","    print(\"Loaded model from disk\")\n","    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    # Load History\n","    proposed_model_1_history = loadHist(path_to_save + model_name + \"_history.json\")\n","else:\n","    proposed_model_1 = model_import_file.proposed_model_1(input_shape,n_classes)\n","    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    proposed_model_1_history = proposed_model_1.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n","\n","    # Save Model\n","    model_json = proposed_model_1.to_json()\n","    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","    proposed_model_1.save_weights(path_to_save + model_name + \".h5\")\n","    # Save History\n","    saveHist(path_to_save + model_name + \"_history.json\",proposed_model_1_history)\n","    print(\"Saved model to disk\")\n","\n","proposed_model_1_eval = proposed_model_1.evaluate(X_test_numpy_array,y_test)\n","\n","model_history.append(proposed_model_1_history)\n","\n","file_path = path_to_save\n","model_name = \"proposed_model_1\"\n","evaluation_metrics_import_file.save_conf_matrix(proposed_model_1,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n","evaluation_metrics_import_file.print_evaluation_metrics(proposed_model_1,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n","\n","# For deleting files if not wanted\n","# model_name = \"proposed_model_1\"\n","want_to_delete = False\n","if want_to_delete:\n","    os.remove(path_to_save + model_name + '.json')\n","    os.remove(path_to_save + model_name + '.h5')\n","    os.remove(path_to_save + model_name + \"history.json\")\n","\n","model_name = \"soyNet\"\n","print(\"SoyNet Model\")\n","if os.path.exists(path_to_save + model_name + '.json'):\n","    json_file = open(path_to_save + model_name + '.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    soyNet_model = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    soyNet_model.load_weights(path_to_save + model_name + \".h5\")\n","    print(\"Loaded model from disk\")\n","    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    # Load History\n","    soyNet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n","else:\n","    soyNet_model = model_import_file.soyNet(input_shape,n_classes)\n","    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    soyNet_model_history = soyNet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n","\n","    # Save Model\n","    model_json = soyNet_model.to_json()\n","    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","    soyNet_model.save_weights(path_to_save + model_name + \".h5\")\n","    # Save History\n","    saveHist(path_to_save + model_name + \"_history.json\",soyNet_model_history)\n","    print(\"Saved model to disk\")\n","\n","soyNet_model_eval = soyNet_model.evaluate(X_test_numpy_array,y_test)\n","\n","# For deleting files if not wanted\n","model_name = \"soyNet\"\n","want_to_delete = False\n","if want_to_delete:\n","  os.remove(path_to_save + model_name + '.json')\n","  os.remove(path_to_save + model_name + '.h5')\n","  os.remove(path_to_save + model_name + \"history.json\")\n","\n","model_history.append(soyNet_model_history)\n","\n","file_path = path_to_save\n","model_name = \"soyNet\"\n","evaluation_metrics_import_file.save_conf_matrix(soyNet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n","evaluation_metrics_import_file.print_evaluation_metrics(soyNet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n","\n","model_name = \"xception_network\"\n","print(\"Xception Network Model\")\n","if os.path.exists(path_to_save + model_name + '.json'):\n","    json_file = open(path_to_save + model_name + '.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    xception_model = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    xception_model.load_weights(path_to_save + model_name + \".h5\")\n","    print(\"Loaded model from disk\")\n","    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    # Load History\n","    xception_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n","else:\n","    xception_model = model_import_file.xception_network(input_shape,n_classes)\n","    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    xception_model_history = xception_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n","\n","    # Save Model\n","    model_json = xception_model.to_json()\n","    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","    xception_model.save_weights(path_to_save + model_name + \".h5\")\n","    # Save History\n","    saveHist(path_to_save + model_name + \"_history.json\",xception_model_history)\n","    print(\"Saved model to disk\")\n","\n","xception_model_eval = xception_model.evaluate(X_test_numpy_array,y_test)\n","\n","model_history.append(xception_model_history)\n","\n","file_path = path_to_save\n","model_name = \"xception_network\"\n","evaluation_metrics_import_file.save_conf_matrix(xception_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n","evaluation_metrics_import_file.print_evaluation_metrics(xception_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n","\n","model_name = \"densenet_121_network\"\n","print(\"Densenet Model\")\n","if os.path.exists(path_to_save + model_name + '.json'):\n","    json_file = open(path_to_save + model_name + '.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    densenet_121_model = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    densenet_121_model.load_weights(path_to_save + model_name + \".h5\")\n","    print(\"Loaded model from disk\")\n","    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    # Load History\n","    densenet_121_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n","else:\n","    densenet_121_model = model_import_file.densenet_121_network(input_shape,n_classes)\n","    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    densenet_121_model_history = densenet_121_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n","\n","    # Save Model\n","    model_json = densenet_121_model.to_json()\n","    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","    densenet_121_model.save_weights(path_to_save + model_name + \".h5\")\n","    # Save History\n","    saveHist(path_to_save + model_name + \"_history.json\",densenet_121_model_history)\n","    print(\"Saved model to disk\")\n","\n","densenet_121_model_eval = densenet_121_model.evaluate(X_test_numpy_array,y_test)\n","\n","model_history.append(densenet_121_model_history)\n","\n","file_path = path_to_save\n","model_name = \"densenet_121_network\"\n","evaluation_metrics_import_file.save_conf_matrix(densenet_121_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n","evaluation_metrics_import_file.print_evaluation_metrics(densenet_121_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n","\n","model_name = \"alexnet\"\n","print(\"Alexnet Model\")\n","if os.path.exists(path_to_save + model_name + '.json'):\n","    json_file = open(path_to_save + model_name + '.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    alexnet_model = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    alexnet_model.load_weights(path_to_save + model_name + \".h5\")\n","    print(\"Loaded model from disk\")\n","    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    # Load History\n","    alexnet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n","else:\n","    alexnet_model = model_import_file.alexnet(input_shape,n_classes)\n","    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    alexnet_model_history = alexnet_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n","\n","    # Save Model\n","    model_json = alexnet_model.to_json()\n","    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","    alexnet_model.save_weights(path_to_save + model_name + \".h5\")\n","    # Save History\n","    saveHist(path_to_save + model_name + \"_history.json\",alexnet_model_history)\n","    print(\"Saved model to disk\")\n","\n","alexnet_model_eval = alexnet_model.evaluate(X_test_numpy_array,y_test)\n","\n","model_history.append(alexnet_model_history)\n","\n","file_path = path_to_save\n","model_name = \"alexnet\"\n","evaluation_metrics_import_file.save_conf_matrix(alexnet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n","evaluation_metrics_import_file.print_evaluation_metrics(alexnet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n","\n","model_name = \"pretrained_VGG16\"\n","print(\"VGG-16 Model\")\n","if os.path.exists(path_to_save + model_name + '.json'):\n","    json_file = open(path_to_save + model_name + '.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    pretrained_VGG16_model = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    pretrained_VGG16_model.load_weights(path_to_save + model_name + \".h5\")\n","    print(\"Loaded model from disk\")\n","    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    # Load History\n","    pretrained_VGG16_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n","else:\n","    pretrained_VGG16_model = model_import_file.pretrained_VGG16(input_shape,n_classes)\n","    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    pretrained_VGG16_model_history = pretrained_VGG16_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n","\n","    # Save Model\n","    model_json = pretrained_VGG16_model.to_json()\n","    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","    pretrained_VGG16_model.save_weights(path_to_save + model_name + \".h5\")\n","    # Save History\n","    saveHist(path_to_save + model_name + \"_history.json\",pretrained_VGG16_model_history)\n","    print(\"Saved model to disk\")\n","\n","pretrained_VGG16_model_eval = pretrained_VGG16_model.evaluate(X_test_numpy_array,y_test)\n","\n","model_history.append(pretrained_VGG16_model_history)\n","\n","file_path = path_to_save\n","model_name = \"pretrained_VGG16\"\n","evaluation_metrics_import_file.save_conf_matrix(pretrained_VGG16_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n","evaluation_metrics_import_file.print_evaluation_metrics(pretrained_VGG16_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n","\n","model_name = \"pretrained_ResNet50\"\n","print(\"Resnet Model\")\n","if os.path.exists(path_to_save + model_name + '.json'):\n","    json_file = open(path_to_save + model_name + '.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    pretrained_ResNet50_model = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    pretrained_ResNet50_model.load_weights(path_to_save + model_name + \".h5\")\n","    print(\"Loaded model from disk\")\n","    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    # Load History\n","    pretrained_ResNet50_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n","else:\n","    pretrained_ResNet50_model = model_import_file.pretrained_ResNet50(input_shape,n_classes)\n","    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    pretrained_ResNet50_model_history = pretrained_ResNet50_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n","\n","    # Save Model\n","    model_json = pretrained_ResNet50_model.to_json()\n","    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","    pretrained_ResNet50_model.save_weights(path_to_save + model_name + \".h5\")\n","    # Save History\n","    saveHist(path_to_save + model_name + \"_history.json\",pretrained_ResNet50_model_history)\n","    print(\"Saved model to disk\")\n","\n","pretrained_ResNet50_model_eval = pretrained_ResNet50_model.evaluate(X_test_numpy_array,y_test)\n","\n","model_history.append(pretrained_ResNet50_model_history)\n","\n","file_path = path_to_save\n","model_name = \"pretrained_ResNet50\"\n","evaluation_metrics_import_file.save_conf_matrix(pretrained_ResNet50_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n","evaluation_metrics_import_file.print_evaluation_metrics(pretrained_ResNet50_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n","\n","model_name = \"pretrained_EfficientNetB2\"\n","print(\"EfficeintNet-B2 Model\")\n","if os.path.exists(path_to_save + model_name + '.json'):\n","    json_file = open(path_to_save + model_name + '.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    pretrained_EfficientNetB2_model = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    pretrained_EfficientNetB2_model.load_weights(path_to_save + model_name + \".h5\")\n","    print(\"Loaded model from disk\")\n","    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    # Load History\n","    pretrained_EfficientNetB2_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n","else:\n","    pretrained_EfficientNetB2_model = model_import_file.pretrained_EfficientNetB2(input_shape,n_classes)\n","    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n","    pretrained_EfficientNetB2_model_history = pretrained_EfficientNetB2_model.fit(X_train_numpy_array,y_train,batch_size = 4,epochs=epoch)\n","\n","    # Save Model\n","    model_json = pretrained_EfficientNetB2_model.to_json()\n","    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","    pretrained_EfficientNetB2_model.save_weights(path_to_save + model_name + \".h5\")\n","    # Save History\n","    saveHist(path_to_save + model_name + \"_history.json\",pretrained_EfficientNetB2_model_history)\n","    print(\"Saved model to disk\")\n","\n","pretrained_EfficientNetB2_model_eval = pretrained_EfficientNetB2_model.evaluate(X_test_numpy_array,y_test)\n","\n","model_history.append(pretrained_EfficientNetB2_model_history)\n","\n","file_path = path_to_save\n","model_name = \"pretrained_EfficientNetB2\"\n","evaluation_metrics_import_file.save_conf_matrix(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n","evaluation_metrics_import_file.print_evaluation_metrics(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n","\n","evaluation_metrics_import_file.save_of_training_accuracy_vs_epochs(model_history,epoch,file_path+\"training_accuracy_vs_epochs\")\n","evaluation_metrics_import_file.save_of_training_loss_vs_epochs(model_history,epoch,file_path+\"training_loss_vs_epochs\")\n","\n","evaluation_metrics_import_file.func_help()\n","\n","training_accuracy_of_different_models = []\n","testing_accuracy_of_different_models = []\n","model_list = [\"Proposed Model 1\",\"SoyNet Model\",\"Xception Model\",\"Densenet 121 Model\",\"AlexNet Model\",\"VGG16 Model\",\"ResNet 50 Model\",\"EfficientNetB2 Model\"]\n","\n","sz = len(model_history)\n","for i in range(sz):\n","    length = len(model_history[i]['accuracy'])\n","    training_accuracy_of_different_models.append(model_history[i]['accuracy'][length-1])\n","\n","\n","testing_accuracy_of_different_models.append(proposed_model_1_eval[1])\n","testing_accuracy_of_different_models.append(soyNet_model_eval[1])\n","testing_accuracy_of_different_models.append(xception_model_eval[1])\n","testing_accuracy_of_different_models.append(densenet_121_model_eval[1])\n","testing_accuracy_of_different_models.append(alexnet_model_eval[1])\n","testing_accuracy_of_different_models.append(pretrained_VGG16_model_eval[1])\n","testing_accuracy_of_different_models.append(pretrained_ResNet50_model_eval[1])\n","testing_accuracy_of_different_models.append(pretrained_EfficientNetB2_model_eval[1])\n","\n","evaluation_metrics_import_file.save_histogram_for_training_and_testing_accuracy(model_list,training_accuracy_of_different_models,testing_accuracy_of_different_models,file_path+\"training_and_testing_accuracy.jpg\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"181107e1","metadata":{"id":"181107e1"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"cb55b740","metadata":{"id":"cb55b740"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"f30bfb98","metadata":{"id":"f30bfb98"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"name":"grapes_model_without_aug.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}