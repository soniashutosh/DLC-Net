{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 17:05:34.034902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:05:34.034944: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'tomato_data_report/import file')\n",
    "\n",
    "import model_import_file\n",
    "import evaluation_metrics_import_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"dataset\"\n",
    "path_to_save = \"tomato_data_report/data_save_without_aug/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "import json,codecs\n",
    "\n",
    "def saveHist(path,history):\n",
    "    \n",
    "    new_hist = {}\n",
    "    for key in list(history.history.keys()):\n",
    "        new_hist[key]=history.history[key]\n",
    "        if type(history.history[key]) == np.ndarray:\n",
    "            new_hist[key] = history.history[key].tolist()\n",
    "        elif type(history.history[key]) == list:\n",
    "            if  type(history.history[key][0]) == np.float64:\n",
    "                new_hist[key] = list(map(float, history.history[key]))\n",
    "            \n",
    "    print(new_hist)\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(new_hist, file, separators=(',', ':'), sort_keys=True, indent=4) \n",
    "\n",
    "def loadHist(path):\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as file:\n",
    "        n = json.loads(file.read())\n",
    "    return n\n",
    "\n",
    "class_dict = {}\n",
    "\n",
    "counter = 0\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        class_dict[subfolder] = counter\n",
    "        counter += 1\n",
    "    break\n",
    "\n",
    "print(counter)\n",
    "class_dict\n",
    "\n",
    "class_list = []\n",
    "for ele in class_dict:\n",
    "    class_list.append(ele)\n",
    "\n",
    "class_list = [\"Bacterial\",\"Healthy\",\"Septoria\",\"Mosaic\",\"TargetSpot\",\"EarlyBlight\",\"SpiderMites\",\"LateBlight\",\"YellowCurl\",\"LeafMold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train : 18355\n",
      "Size of y_train : 18355\n",
      "Size of X_test : 4585\n",
      "Size of y_test : 4585\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE_X = 200\n",
    "IMG_SIZE_Y = 200\n",
    "\n",
    "total_training_images_classwise = {}\n",
    "total_testing_images_classwise ={}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for folder in os.listdir(path_to_data):\n",
    "    path = os.path.join(path_to_data,folder)\n",
    "    for subfolder in os.listdir(path):\n",
    "        path_to_subfolder = os.path.join(path,subfolder)\n",
    "        key = subfolder\n",
    "        number_of_image_in_folder = len(os.listdir(path_to_subfolder))\n",
    "        if folder == \"valid\":\n",
    "            total_testing_images_classwise[subfolder] = number_of_image_in_folder\n",
    "        else:\n",
    "            total_training_images_classwise[subfolder] = number_of_image_in_folder\n",
    "        count = 0\n",
    "        for img in os.listdir(path_to_subfolder):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path_to_subfolder,img))\n",
    "                if folder == \"valid\":\n",
    "                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                    X_test.append(resized_array)\n",
    "                    y_test.append(class_dict[subfolder])\n",
    "                else:\n",
    "                    resized_array = cv2.resize(img_array, (IMG_SIZE_X,IMG_SIZE_Y))\n",
    "                    X_train.append(resized_array)\n",
    "                    y_train.append(class_dict[subfolder])\n",
    "            except Exception as e:\n",
    "                print(\"Exception is : \"+e)\n",
    "                pass\n",
    "            count += 1\n",
    "            \n",
    "print(\"Size of X_train : \" + str(len(X_train)))\n",
    "print(\"Size of y_train : \" + str(len(y_train)))\n",
    "print(\"Size of X_test : \" + str(len(X_test)))\n",
    "print(\"Size of y_test : \" + str(len(y_test)))\n",
    "\n",
    "file_path = path_to_save\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_data(class_list,total_training_images_classwise,total_testing_images_classwise,file_path+\"histogram_for_training_and_testing_data.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 17:06:20.330571: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:06:20.330704: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:06:20.330760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:06:20.330809: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:06:20.330872: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:06:20.330939: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:06:20.330987: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:06:20.331037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.0/lib64\n",
      "2022-05-21 17:06:20.331048: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-21 17:06:20.331450: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 17:06:24.445186: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2200800000 exceeds 10% of free system memory.\n",
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 124s 832ms/step - loss: 0.1071 - accuracy: 0.9658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 17:08:32.517053: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2200800000 exceeds 10% of free system memory.\n",
      "2022-05-21 17:10:43.762409: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2200800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 |   Class 5 |   Class 6 |   Class 7 |   Class 8 |   Class 9 |   Class 10 | Measure                          |\n",
      "+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+==================================+\n",
      "| 99.5856   | 99.6947   | 99.1276   | 99.6947   | 98.5387   | 98.8877   | 99.193    | 98.9967   | 99.7819   |  99.651    | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.414395 |  0.305344 |  0.87241  |  0.305344 |  1.46129  |  1.11232  |  0.806979 |  1.00327  |  0.218103 |   0.348964 | Error                            |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.981043 |  0.993658 |  0.962617 |  0.980088 |  0.911392 |  0.921415 |  0.97381  |  0.968539 |  0.98583  |   0.985043 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.974118 |  0.977131 |  0.944954 |  0.988839 |  0.945295 |  0.977083 |  0.94023  |  0.930886 |  0.993878 |   0.980851 | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.974118 |  0.977131 |  0.944954 |  0.988839 |  0.945295 |  0.977083 |  0.94023  |  0.930886 |  0.993878 |   0.980851 | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.998077 |  0.999269 |  0.996144 |  0.997825 |  0.989826 |  0.990256 |  0.997349 |  0.996604 |  0.998291 |   0.998299 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.977568 |  0.985325 |  0.953704 |  0.984444 |  0.928034 |  0.948433 |  0.956725 |  0.949339 |  0.989837 |   0.982942 | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.989523 |  0.996459 |  0.979237 |  0.988917 |  0.9498   |  0.955215 |  0.985509 |  0.982471 |  0.992041 |   0.991649 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.975344 |  0.983685 |  0.949156 |  0.982786 |  0.920634 |  0.942917 |  0.952625 |  0.944251 |  0.988635 |   0.981035 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "SoyNet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 17:12:47.626961: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2200800000 exceeds 10% of free system memory.\n",
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 14s 92ms/step - loss: 0.8866 - accuracy: 0.6914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 17:13:05.624283: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2200800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 |   Class 5 |   Class 6 |   Class 7 |   Class 8 |   Class 9 |   Class 10 | Measure                          |\n",
      "+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+==================================+\n",
      "| 94.5256   | 97.0774   | 91.7557   | 96.4013   | 93.1734   | 90.7525   | 91.9738   | 92.868    | 96.0087   |  93.7405   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  5.47437  |  2.92257  |  8.24427  |  3.59869  |  6.82661  |  9.24755  |  8.02617  |  7.13195  |  3.99128  |   6.25954  | Error                            |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.668605 |  0.884701 |  0.56682  |  0.787018 |  0.641732 |  0.562222 |  0.573626 |  0.694286 |  0.805169 |   0.715294 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.811765 |  0.829522 |  0.56422  |  0.866071 |  0.713348 |  0.527083 |  0.6      |  0.524838 |  0.826531 |   0.646809 | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.811765 |  0.829522 |  0.56422  |  0.866071 |  0.713348 |  0.527083 |  0.6      |  0.524838 |  0.826531 |   0.646809 | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.958894 |  0.987329 |  0.954688 |  0.974619 |  0.955911 |  0.95201  |  0.953253 |  0.974042 |  0.976068 |   0.970595 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.733262 |  0.856223 |  0.565517 |  0.824655 |  0.675648 |  0.544086 |  0.586517 |  0.597786 |  0.81571  |   0.67933  | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.8007   |  0.934607 |  0.73562  |  0.87581  |  0.783223 |  0.731602 |  0.739467 |  0.822352 |  0.88651  |   0.833223 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.714241 |  0.842754 |  0.539765 |  0.809034 |  0.65077  |  0.516377 |  0.560582 |  0.580145 |  0.797517 |   0.656633 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "Xception Network\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 110s 754ms/step - loss: 0.1789 - accuracy: 0.9413\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 |   Class 5 |   Class 6 |   Class 7 |   Class 8 |   Class 9 |   Class 10 | Measure                          |\n",
      "+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+==================================+\n",
      "| 99.3893   | 99.4984   | 99.2148   | 99.8037   | 97.6663   | 97.5791   | 98.5823   | 97.928    | 99.2803   |  99.3239   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.610687 |  0.501636 |  0.785169 |  0.196292 |  2.3337   |  2.42094  |  1.41767  |  2.07197  |  0.719738 |   0.676118 | Error                            |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.956322 |  0.981092 |  0.983092 |  0.984547 |  0.877155 |  0.848771 |  0.926267 |  0.929907 |  0.983087 |   0.958246 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.978824 |  0.970894 |  0.933486 |  0.995536 |  0.890591 |  0.935417 |  0.924138 |  0.859611 |  0.94898  |   0.976596 | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.978824 |  0.970894 |  0.933486 |  0.995536 |  0.890591 |  0.935417 |  0.924138 |  0.859611 |  0.94898  |   0.976596 | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.995433 |  0.997807 |  0.998313 |  0.998308 |  0.986192 |  0.980512 |  0.992289 |  0.992722 |  0.998046 |   0.99514  | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.967442 |  0.975967 |  0.957647 |  0.990011 |  0.883822 |  0.88999  |  0.925201 |  0.893378 |  0.965732 |   0.967334 | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.975681 |  0.989414 |  0.990673 |  0.991404 |  0.930077 |  0.912266 |  0.95871  |  0.960801 |  0.990538 |   0.976519 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.964242 |  0.973249 |  0.953839 |  0.988946 |  0.872379 |  0.878914 |  0.917958 |  0.883836 |  0.962009 |   0.963726 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "Densenet 121 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 127s 858ms/step - loss: 0.1558 - accuracy: 0.9490\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 |   Class 5 |   Class 6 |   Class 7 |   Class 8 |   Class 9 |   Class 10 | Measure                          |\n",
      "+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+==================================+\n",
      "| 99.2148   | 99.6728   | 99.1276   | 99.6292   | 98.1897   | 98.1897   | 98.5823   | 98.3424   | 99.4984   |  99.3457   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.785169 |  0.327154 |  0.87241  |  0.370774 |  1.81025  |  1.81025  |  1.41767  |  1.65758  |  0.501636 |   0.654308 | Error                            |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.943052 |  0.987448 |  0.964789 |  0.979955 |  0.904762 |  0.90593  |  0.920455 |  0.934831 |  0.983437 |   0.964135 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.974118 |  0.981289 |  0.942661 |  0.982143 |  0.914661 |  0.922917 |  0.931034 |  0.898488 |  0.969388 |   0.97234  | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.974118 |  0.981289 |  0.942661 |  0.982143 |  0.914661 |  0.922917 |  0.931034 |  0.898488 |  0.969388 |   0.97234  | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.99399  |  0.998538 |  0.996385 |  0.997825 |  0.989341 |  0.988794 |  0.991566 |  0.992965 |  0.998046 |   0.995869 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.958333 |  0.984359 |  0.953596 |  0.981048 |  0.909684 |  0.914345 |  0.925714 |  0.9163   |  0.976362 |   0.96822  | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.968186 |  0.992977 |  0.980459 |  0.98885  |  0.946107 |  0.946456 |  0.955349 |  0.96346  |  0.990715 |   0.979873 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.954299 |  0.982567 |  0.949075 |  0.979032 |  0.900544 |  0.905126 |  0.918473 |  0.908061 |  0.973651 |   0.964696 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "Alexnet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 20s 135ms/step - loss: 0.2938 - accuracy: 0.9084\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 |   Class 5 |   Class 6 |   Class 7 |   Class 8 |   Class 9 |   Class 10 | Measure                          |\n",
      "+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+==================================+\n",
      "| 99.1058   | 99.3021   | 97.5136   | 99.5856   | 97.012    | 96.2268   | 97.7317   | 97.7754   | 99.193    |  98.2334   | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.89422  |  0.697928 |  2.48637  |  0.414395 |  2.988    |  3.77317  |  2.26827  |  2.22465  |  0.806979 |   1.76663  | Error                            |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.948598 |  0.95723  |  0.892683 |  0.96732  |  0.849345 |  0.802761 |  0.878719 |  0.913043 |  0.957576 |   0.920086 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.955294 |  0.977131 |  0.83945  |  0.991071 |  0.851204 |  0.847917 |  0.882759 |  0.861771 |  0.967347 |   0.906383 | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.955294 |  0.977131 |  0.83945  |  0.991071 |  0.851204 |  0.847917 |  0.882759 |  0.861771 |  0.967347 |   0.906383 | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.994712 |  0.994883 |  0.989395 |  0.996374 |  0.983285 |  0.975639 |  0.987229 |  0.990781 |  0.994872 |   0.991009 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.951934 |  0.967078 |  0.865248 |  0.979052 |  0.850273 |  0.824721 |  0.880734 |  0.886667 |  0.962437 |   0.913183 | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.971381 |  0.975875 |  0.939796 |  0.98174  |  0.913864 |  0.884989 |  0.931395 |  0.951118 |  0.976046 |   0.954889 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.947246 |  0.96335  |  0.853821 |  0.976867 |  0.836162 |  0.807579 |  0.869697 |  0.876115 |  0.958096 |   0.904233 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "VGG16 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 214s 1s/step - loss: 2.0955 - accuracy: 0.4469\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 |   Class 5 |   Class 6 |   Class 7 |   Class 8 |   Class 9 |   Class 10 | Measure                          |\n",
      "+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+==================================+\n",
      "| 89.6619   | 88.8768   | 85.4526   | 93.1952   | 83.7732   | 88.7241   | 90.0327   | 88.8986   | 93.5224   |  87.241    | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "| 10.3381   | 11.1232   | 14.5474   |  6.8048   | 16.2268   | 11.2759   |  9.96728  | 11.1014   |  6.47764  |  12.759    | Error                            |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.462935 |  0.461126 |  0.322034 |  0.625926 |  0.280245 |  0.439344 |  0.474537 |  0.399123 |  0.914163 |   0.387476 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.72     |  0.357588 |  0.479358 |  0.754464 |  0.400438 |  0.279167 |  0.471264 |  0.196544 |  0.434694 |   0.421277 | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.72     |  0.357588 |  0.479358 |  0.754464 |  0.400438 |  0.279167 |  0.471264 |  0.196544 |  0.434694 |   0.421277 | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.914663 |  0.951023 |  0.89395  |  0.951172 |  0.886143 |  0.958343 |  0.945301 |  0.966764 |  0.995116 |   0.923937 | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.563536 |  0.40281  |  0.385253 |  0.684211 |  0.32973  |  0.341401 |  0.472895 |  0.263386 |  0.589212 |   0.40367  | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.650715 |  0.662225 |  0.536547 |  0.771598 |  0.498335 |  0.648878 |  0.669761 |  0.621174 |  0.953781 |   0.598333 | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "|  0.543603 |  0.381261 |  0.360511 |  0.660988 |  0.304072 |  0.328802 |  0.446871 |  0.263539 |  0.608679 |   0.375145 | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------------------------------+\n",
      "Resnet Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 122s 838ms/step - loss: 2.2810 - accuracy: 0.1760\n",
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|    Class 1 |    Class 2 |     Class 3 |   Class 4 |   Class 5 |    Class 6 |   Class 7 |    Class 8 |   Class 9 |   Class 10 | Measure                          |\n",
      "+============+============+=============+===========+===========+============+===========+============+===========+============+==================================+\n",
      "| 90.578     | 84.7546    | 90.1418     | 80.5234   | 75.3108   | 84.4711    | 84.0131   | 85.4962    | 71.7339   | 88.1788    | Accuracy                         |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|  9.42203   | 15.2454    |  9.85823    | 19.4766   | 24.6892   | 15.5289    | 15.9869   | 14.5038    | 28.2661   | 11.8212    | Error                            |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|  0.347826  |  0.134228  |  0.1        |  0.17037  |  0.173282 |  0.120915  |  0.153488 |  0.125926  |  0.220139 |  0.1       | Precision                        |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|  0.0188235 |  0.0831601 |  0.00458716 |  0.256696 |  0.391685 |  0.0770833 |  0.151724 |  0.0734341 |  0.646939 |  0.0191489 | Recall                           |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|  0.0188235 |  0.0831601 |  0.00458716 |  0.256696 |  0.391685 |  0.0770833 |  0.151724 |  0.0734341 |  0.646939 |  0.0191489 | Sensitivity                      |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|  0.996394  |  0.937135  |  0.995662   |  0.864636 |  0.79312  |  0.93447   |  0.912289 |  0.942746  |  0.725763 |  0.980316  | Specificity                      |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|  0.0357143 |  0.102696  |  0.00877193 |  0.204809 |  0.240268 |  0.0941476 |  0.152601 |  0.0927694 |  0.328497 |  0.0321429 | F1 Score                         |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|  0.588704  |  0.354669  |  0.315541   |  0.383808 |  0.37072  |  0.336142  |  0.3742   |  0.344552  |  0.399711 |  0.3131    | G Mean                           |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "|  0.0779229 |  0.0969894 |  0.0213535  |  0.18589  |  0.222531 |  0.0884715 |  0.139138 |  0.0887358 |  0.312215 |  0.0414744 | Matthews Correlation Coefficient |\n",
      "+------------+------------+-------------+-----------+-----------+------------+-----------+------------+-----------+------------+----------------------------------+\n",
      "EfficientNet B2 Model\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/envs/aman/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 83s 551ms/step - loss: 2.3025 - accuracy: 0.1099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tomato_data_report/import file/evaluation_metrics_import_file.py:91: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = (TP) / (TP + FP)\n",
      "tomato_data_report/import file/evaluation_metrics_import_file.py:120: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return num/(np.sqrt(den))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Report \n",
      "\n",
      "\n",
      "\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "|   Class 1 |   Class 2 |   Class 3 |   Class 4 |   Class 5 |   Class 6 |     Class 7 |   Class 8 |   Class 9 |   Class 10 | Measure                          |\n",
      "+===========+===========+===========+===========+===========+===========+=============+===========+===========+============+==================================+\n",
      "|  90.7306  | 40.5671   |  90.4907  |  90.229   |  90.0327  |   89.5311 | 86.9575     |   89.9019 |    89.313 | 64.2312    | Accuracy                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "|   9.26936 | 59.4329   |   9.50927 |   9.77099 |   9.96728 |   10.4689 | 13.0425     |   10.0981 |    10.687 | 35.7688    | Error                            |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "| nan       |  0.129213 | nan       | nan       | nan       |  nan      |  0.011976   |  nan      |   nan     |  0.0797414 | Precision                        |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "|   0       |  0.81289  |   0       |   0       |   0       |    0      |  0.0045977  |    0      |     0     |  0.23617   | Recall                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "|   0       |  0.81289  |   0       |   0       |   0       |    0      |  0.0045977  |    0      |     0     |  0.23617   | Sensitivity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "|   1       |  0.357943 |   1       |   1       |   1       |    1      |  0.960241   |    1      |     1     |  0.6887    | Specificity                      |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "| nan       |  0.222983 | nan       | nan       | nan       |  nan      |  0.00664452 |  nan      |   nan     |  0.119227  | F1 Score                         |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "| nan       |  0.215061 | nan       | nan       | nan       |  nan      |  0.107238   |  nan      |   nan     |  0.234346  | G Mean                           |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "| inf       |  0.187386 | inf       | inf       | inf       |  inf      |  0.00713809 |  inf      |   inf     |  0.106978  | Matthews Correlation Coefficient |\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-------------+-----------+-----------+------------+----------------------------------+\n",
      "save_histogram_for_training_and_testing_accuracy(model_list,training_acc,testing_acc,file_path)\n",
      "save_of_training_accuracy_vs_epochs(model_history,epochs,file_path)\n",
      "save_of_training_loss_vs_epochs(model_history,epochs,file_path)\n",
      "print_evaluation_metrics(model,X_test_numpy_array,y_test,file_path)\n",
      "save_conf_matrix(model,X_test,y_test,class_list,file_path)\n",
      "save_histogram_for_training_and_testing_data(class_list,training_data_dict,testing_data_dict,file_path)\n"
     ]
    }
   ],
   "source": [
    "X_train_numpy_array = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test_numpy_array = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_numpy_array = X_train_numpy_array/255\n",
    "X_test_numpy_array = X_test_numpy_array/255\n",
    "\n",
    "input_shape = IMG_SIZE_X, IMG_SIZE_Y, 3\n",
    "n_classes = counter\n",
    "epoch = 50\n",
    "# Adagrad\n",
    "adagrad_opt = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.0005,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    name=\"Adagrad\",\n",
    ")\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "batch = 64\n",
    "\n",
    "model_history = []\n",
    "\n",
    "print(\"Proposed Model 1\")\n",
    "model_name = \"proposed_model_1\"\n",
    "\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    proposed_model_1 = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    proposed_model_1.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    proposed_model_1_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    proposed_model_1 = model_import_file.proposed_model_1(input_shape,n_classes)\n",
    "    proposed_model_1.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    proposed_model_1_history = proposed_model_1.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = proposed_model_1.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    proposed_model_1.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",proposed_model_1_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "proposed_model_1_eval = proposed_model_1.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(proposed_model_1_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"proposed_model_1\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(proposed_model_1,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(proposed_model_1,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "print(\"SoyNet Model\")\n",
    "model_name = \"soyNet\"\n",
    "\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    soyNet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    soyNet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    soyNet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    soyNet_model = model_import_file.soyNet(input_shape,n_classes)\n",
    "    soyNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    soyNet_model_history = soyNet_model.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = soyNet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    soyNet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",soyNet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "soyNet_model_eval = soyNet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "# For deleting files if not wanted\n",
    "model_name = \"soyNet\"\n",
    "want_to_delete = False\n",
    "if want_to_delete:\n",
    "  os.remove(path_to_save + model_name + '.json')\n",
    "  os.remove(path_to_save + model_name + '.h5')\n",
    "  os.remove(path_to_save + model_name + \"history.json\")\n",
    "\n",
    "model_history.append(soyNet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"soyNet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(soyNet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(soyNet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "print(\"Xception Network\")\n",
    "model_name = \"xception_network\"\n",
    "\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    xception_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    xception_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    xception_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    xception_model = model_import_file.xception_network(input_shape,n_classes)\n",
    "    xception_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    xception_model_history = xception_model.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = xception_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    xception_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",xception_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "xception_model_eval = xception_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(xception_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"xception_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(xception_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(xception_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "print(\"Densenet 121 Model\")\n",
    "model_name = \"densenet_121_network\"\n",
    "\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    densenet_121_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    densenet_121_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    densenet_121_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    densenet_121_model = model_import_file.densenet_121_network(input_shape,n_classes)\n",
    "    densenet_121_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    densenet_121_model_history = densenet_121_model.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = densenet_121_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    densenet_121_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",densenet_121_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "densenet_121_model_eval = densenet_121_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(densenet_121_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"densenet_121_network\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(densenet_121_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(densenet_121_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "print(\"Alexnet Model\")\n",
    "model_name = \"alexnet\"\n",
    "\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    alexnet_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    alexnet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    alexnet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    alexnet_model = model_import_file.alexnet(input_shape,n_classes)\n",
    "    alexnet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    alexnet_model_history = alexnet_model.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = alexnet_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    alexnet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",alexnet_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "alexnet_model_eval = alexnet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(alexnet_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"alexnet\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(alexnet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(alexnet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "print(\"VGG16 Model\")\n",
    "model_name = \"pretrained_VGG16\"\n",
    "\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_VGG16_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_VGG16_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_VGG16_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_VGG16_model = model_import_file.pretrained_VGG16(input_shape,n_classes)\n",
    "    pretrained_VGG16_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_VGG16_model_history = pretrained_VGG16_model.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_VGG16_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_VGG16_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_VGG16_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_VGG16_model_eval = pretrained_VGG16_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_VGG16_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_VGG16\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_VGG16_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_VGG16_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "print(\"Resnet Model\")\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_ResNet50_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_ResNet50_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_ResNet50_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_ResNet50_model = model_import_file.pretrained_ResNet50(input_shape,n_classes)\n",
    "    pretrained_ResNet50_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_ResNet50_model_history = pretrained_ResNet50_model.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_ResNet50_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_ResNet50_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_ResNet50_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_ResNet50_model_eval = pretrained_ResNet50_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_ResNet50_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_ResNet50\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_ResNet50_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_ResNet50_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "print(\"EfficientNet B2 Model\")\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "\n",
    "if os.path.exists(path_to_save + model_name + '.json'):\n",
    "    json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pretrained_EfficientNetB2_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    pretrained_EfficientNetB2_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    # Load History\n",
    "    pretrained_EfficientNetB2_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "else:\n",
    "    pretrained_EfficientNetB2_model = model_import_file.pretrained_EfficientNetB2(input_shape,n_classes)\n",
    "    pretrained_EfficientNetB2_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "    pretrained_EfficientNetB2_model_history = pretrained_EfficientNetB2_model.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    model_json = pretrained_EfficientNetB2_model.to_json()\n",
    "    with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    pretrained_EfficientNetB2_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "    # Save History\n",
    "    saveHist(path_to_save + model_name + \"_history.json\",pretrained_EfficientNetB2_model_history)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "pretrained_EfficientNetB2_model_eval = pretrained_EfficientNetB2_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "model_history.append(pretrained_EfficientNetB2_model_history)\n",
    "\n",
    "file_path = path_to_save\n",
    "model_name = \"pretrained_EfficientNetB2\"\n",
    "evaluation_metrics_import_file.save_conf_matrix(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "evaluation_metrics_import_file.print_evaluation_metrics(pretrained_EfficientNetB2_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "# print(\"Mobilenet Model\")\n",
    "# model_name = \"pretrained_MobileNet\"\n",
    "\n",
    "# if os.path.exists(path_to_save + model_name + '.json'):\n",
    "#     json_file = open(path_to_save + model_name + '.json', 'r')\n",
    "#     loaded_model_json = json_file.read()\n",
    "#     json_file.close()\n",
    "#     pretrained_MobileNet_model = model_from_json(loaded_model_json)\n",
    "#     # load weights into new model\n",
    "#     pretrained_MobileNet_model.load_weights(path_to_save + model_name + \".h5\")\n",
    "#     print(\"Loaded model from disk\")\n",
    "#     pretrained_MobileNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "#     # Load History\n",
    "#     pretrained_MobileNet_model_history = loadHist(path_to_save + model_name + \"_history.json\")\n",
    "# else:\n",
    "#     pretrained_MobileNet_model = model_import_file.pretrained_MobileNet(input_shape,n_classes)\n",
    "#     pretrained_MobileNet_model.compile(optimizer=adagrad_opt,loss = loss , metrics=['accuracy'])\n",
    "#     pretrained_MobileNet_model_history = pretrained_MobileNet_model.fit(X_train_numpy_array,y_train,batch_size = batch,epochs=epoch)\n",
    "\n",
    "#     # Save Model\n",
    "#     model_json = pretrained_MobileNet_model.to_json()\n",
    "#     with open(path_to_save + model_name + \".json\", \"w\") as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     pretrained_MobileNet_model.save_weights(path_to_save + model_name + \".h5\")\n",
    "#     # Save History\n",
    "#     saveHist(path_to_save + model_name + \"_history.json\",pretrained_MobileNet_model_history)\n",
    "#     print(\"Saved model to disk\")\n",
    "\n",
    "# pretrained_MobileNet_model_eval = pretrained_MobileNet_model.evaluate(X_test_numpy_array,y_test)\n",
    "\n",
    "# model_history.append(pretrained_MobileNet_model_history)\n",
    "\n",
    "# file_path = path_to_save\n",
    "# model_name = \"pretrained_MobileNet\"\n",
    "# evaluation_metrics_import_file.save_conf_matrix(pretrained_MobileNet_model,X_test_numpy_array,y_test,class_list,file_path+model_name+\"conf_matrix.jpg\")\n",
    "# evaluation_metrics_import_file.print_evaluation_metrics(pretrained_MobileNet_model,X_test_numpy_array,y_test,file_path+model_name+\"evaluation_metrics.xlsx\")\n",
    "\n",
    "evaluation_metrics_import_file.save_of_training_accuracy_vs_epochs(model_history,epoch,file_path+\"training_accuracy_vs_epochs\")\n",
    "evaluation_metrics_import_file.save_of_training_loss_vs_epochs(model_history,epoch,file_path+\"training_loss_vs_epochs\")\n",
    "\n",
    "evaluation_metrics_import_file.func_help()\n",
    "\n",
    "training_accuracy_of_different_models = []\n",
    "testing_accuracy_of_different_models = []\n",
    "model_list = [\"Proposed\",\"SoyNet\",\"Xception\",\"Densenet 121\",\"AlexNet\",\"VGG16\",\"ResNet 50\",\"EfficientNetB2\"]\n",
    "\n",
    "sz = len(model_history)\n",
    "for i in range(sz):\n",
    "    length = len(model_history[i]['accuracy'])\n",
    "    training_accuracy_of_different_models.append(model_history[i]['accuracy'][length-1])\n",
    "\n",
    "\n",
    "# testing_accuracy_of_different_models.append(proposed_model_1_eval[1])\n",
    "testing_accuracy_of_different_models.append(proposed_model_2_eval[1])\n",
    "testing_accuracy_of_different_models.append(soyNet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(xception_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(densenet_121_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(alexnet_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_VGG16_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_ResNet50_model_eval[1])\n",
    "testing_accuracy_of_different_models.append(pretrained_EfficientNetB2_model_eval[1])\n",
    "# testing_accuracy_of_different_models.append(pretrained_MobileNet_model_eval[1])\n",
    "\n",
    "evaluation_metrics_import_file.save_histogram_for_training_and_testing_accuracy(model_list,training_accuracy_of_different_models,testing_accuracy_of_different_models,file_path+\"training_and_testing_accuracy.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
